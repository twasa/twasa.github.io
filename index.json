[
{
	"uri": "https://blog.twasa.cf/post/wsl/",
	"title": "Windows Subsystem for Linux install and configuration",
	"tags": [],
	"description": "",
	"content": " Introduction Windows Subsystem for Linux and Beautiful shell Required  Windows 10 Anniversary Update build 14316 or later! Administrator permission cmder(Options, for good color schema and powerline fonts support)  Install Subsystem Linux using powershell  Install Subsystem  powershell -command Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux   Install Ubuntu from Microsoft Store  install fontconfg on Ubuntu apt-get install fontconfig  install PowerlineSymbols wget https://github.com/powerline/powerline/raw/develop/font/PowerlineSymbols.otf wget https://github.com/powerline/powerline/raw/develop/font/10-powerline-symbols.conf mkdir -p ~/.local/share/fonts/ mkdir -p ~/.config/fontconfig/conf.d/ mv PowerlineSymbols.otf ~/.local/share/fonts/ fc-cache -vf ~/.local/share/fonts/ mv 10-powerline-symbols.conf ~/.config/fontconfig/conf.d/  Subsystem Linux install zsh, oh-my-zsh, theme and configurations  install zsh  apt install zsh   install oh-my-zsh  sh -c \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\u0026quot;   install oh-my-zsh theme: Powerlevel9k  git clone https://github.com/bhilburn/powerlevel9k.git ~/.oh-my-zsh/custom/themes/powerlevel9k   oh-my-zsh configurations in ~/.zshrc  export ZSH=\u0026quot;/home/william/.oh-my-zsh\u0026quot; ZSH_THEME=\u0026quot;powerlevel9k/powerlevel9k\u0026quot; POWERLEVEL9K_MODE='nerdfont-complete' POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(context dir dir_writable vcs vi_mode) POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status) POWERLEVEL9K_PROMPT_ON_NEWLINE=true plugins=( git ) source $ZSH/oh-my-zsh.sh  cmder configurations  Main console font: DejaVuSansMono Nerd Font\n Alternative font: DejaVuSansMono Nerd Font\n Add a Tsks WSL in Startup \u0026gt; Tasks\n WSL task parameters\n  -icon \u0026quot;%USERPROFILE%\\AppData\\Local\\lxss\\bash.ico\u0026quot;   WSL task commands  set \u0026quot;PATH=%ConEmuBaseDirShort%\\wsl;%PATH%\u0026quot; \u0026amp; %ConEmuBaseDirShort%\\conemu-cyg-64.exe --wsl -cur_console:pm -t zsh -l   Startup Specified named task: {WSL}  Result "
},
{
	"uri": "https://blog.twasa.cf/post/wildcard_ssl_in_synology_nas/",
	"title": "How to install and auto update Let&#39;s encrypt wildcard certs on Synology NAS with cloudflare DNS API",
	"tags": [],
	"description": "",
	"content": " requirements  Synology ssh enabled and sudo permission Cloudflare API key  Installation of acme.sh $ sudo -i $ wget https://github.com/Neilpang/acme.sh/archive/master.tar.gz $ tar xvf master.tar.gz $ cd acme.sh-master/ $ ./acme.sh --install --nocron --home /usr/local/share/acme.sh --accountemail \u0026quot;email@example.com\u0026quot;  Configuring DNS For CloudFlare, we will set two environment variables that acme.sh (specifically, the dns_cf script from the dnsapi subdirectory) will read to set the DNS record.\nexport CF_Key=\u0026quot;MY_SECRET_KEY_SUCH_SECRET\u0026quot; export CF_Email=\u0026quot;email@example.com\u0026quot;  Creating the certificate $ cd /usr/local/share/acme.sh $ export CERT_DOMAIN=\u0026quot;your-domain.tld\u0026quot; $ export CERT_DNS=\u0026quot;dns_cf\u0026quot; $ ./acme.sh --issue -d \u0026quot;$CERT_DOMAIN\u0026quot; --dns \u0026quot;$CERT_DNS\u0026quot; \\ --certpath /usr/syno/etc/certificate/system/default/cert.pem \\ --keypath /usr/syno/etc/certificate/system/default/privkey.pem \\ --fullchainpath /usr/syno/etc/certificate/system/default/fullchain.pem \\ --reloadcmd \u0026quot;/usr/syno/sbin/synoservicectl --reload nginx\u0026quot; \\ --dnssleep 20  Configuring Certificate Renewal by /etc/crontab 0 10 2 * * root /usr/local/share/acme.sh/acme.sh --cron --home /usr/local/share/acme.sh/  "
},
{
	"uri": "https://blog.twasa.cf/post/consul/",
	"title": "Consul",
	"tags": [],
	"description": "",
	"content": " Consul Description Consul has multiple components, but as a whole, it is a tool for discovering and configuring services in your infrastructure.\nFeatures  Service Discovery: Clients of Consul can provide a service, such as api or mysql, and other clients can use Consul to discover providers of a given service. Using either DNS or HTTP, applications can easily find the services they depend upon. Health Checking: Consul clients can provide any number of health checks, either associated with a given service (\u0026ldquo;is the webserver returning 200 OK\u0026rdquo;), or with the local node (\u0026ldquo;is memory utilization below 90%\u0026rdquo;). This information can be used by an operator to monitor cluster health, and it is used by the service discovery components to route traffic away from unhealthy hosts. KV Store: Applications can make use of Consul\u0026rsquo;s hierarchical key/value store for any number of purposes, including dynamic configuration, feature flagging, coordination, leader election, and more. The simple HTTP API makes it easy to use. Multi Datacenter: Consul supports multiple datacenters out of the box. This means users of Consul do not have to worry about building additional layers of abstraction to grow to multiple regions.  Glossary  Agent - An agent is the long running daemon on every member of the Consul cluster. It is started by running consul agent. The agent is able to run in either client or server mode. Since all nodes must be running an agent, it is simpler to refer to the node as being either a client or server, but there are other instances of the agent. All agents can run the DNS or HTTP interfaces, and are responsible for running checks and keeping services in sync. Client - A client is an agent that forwards all RPCs to a server. The client is relatively stateless. The only background activity a client performs is taking part in the LAN gossip pool. This has a minimal resource overhead and consumes only a small amount of network bandwidth. Server - A server is an agent with an expanded set of responsibilities including participating in the Raft quorum, maintaining cluster state, responding to RPC queries, exchanging WAN gossip with other datacenters, and forwarding queries to leaders or remote datacenters. Datacenter - While the definition of a datacenter seems obvious, there are subtle details that must be considered. For example, in EC2, are multiple availability zones considered to comprise a single datacenter? We define a datacenter to be a networking environment that is private, low latency, and high bandwidth. This excludes communication that would traverse the public internet, but for our purposes multiple availability zones within a single EC2 region would be considered part of a single datacenter. Consensus - When used in our documentation we use consensus to mean agreement upon the elected leader as well as agreement on the ordering of transactions. Since these transactions are applied to a finite-state machine, our definition of consensus implies the consistency of a replicated state machine. Consensus is described in more detail on Wikipedia, and our implementation is described here. Gossip - Consul is built on top of Serf which provides a full gossip protocol that is used for multiple purposes. Serf provides membership, failure detection, and event broadcast. Our use of these is described more in the gossip documentation. It is enough to know that gossip involves random node-to-node communication, primarily over UDP. LAN Gossip - Refers to the LAN gossip pool which contains nodes that are all located on the same local area network or datacenter. WAN Gossip - Refers to the WAN gossip pool which contains only servers. These servers are primarily located in different datacenters and typically communicate over the internet or wide area network. RPC - Remote Procedure Call. This is a request / response mechanism allowing a client to make a request of a server.  Ports  Server RPC (Default 8300). This is used by servers to handle incoming requests from other agents. TCP only. Serf LAN (Default 8301). This is used to handle gossip in the LAN. Required by all agents. TCP and UDP. Serf WAN (Default 8302). This is used by servers to gossip over the WAN to other servers. TCP and UDP. CLI RPC (Default 8400). This is used by all agents to handle RPC from the CLI. TCP only. HTTP API (Default 8500). This is used by clients to talk to the HTTP API. TCP only. DNS Interface (Default 8600). Used to resolve DNS queries. TCP and UDP.  Syntax  Usage  consul [--version] [--help] \u0026lt;command\u0026gt; [\u0026lt;args\u0026gt;]   commands  version: Prints the Consul version agent: Runs a Consul agent join: Tell Consul agent to join cluster members: Lists the members of a Consul cluster reload: Triggers the agent to reload configuration files leave: Gracefully leaves the Consul cluster and shuts down validate: Validate config files/directories info: Provides debugging information for operators.  agent command args  datacenter: Datacenter of the agent. dev: Starts the agent in development mode. data-dir: Path to a data directory to store agent state. config-file: A configuration file to load config-dir: Path to a directory to read configuration files from in \u0026lsquo;.json\u0026rsquo;, Can be specified multiple times. node: Name of this node. Must be unique in the cluster. bind: The address that should be bound to for internal cluster communications. By default, this is \u0026ldquo;0.0.0.0\u0026rdquo;, if there was multi private ip, this args must be config http-port: Sets the HTTP API port to listen on. dns-port: DNS port to use. server: used to control if an agent is in server or client mode. When provided, an agent will act as a Consul server. Each Consul cluster must have at least one server and ideally no more than 5 per datacenter. client: The address to which Consul will bind client interfaces, including the HTTP and DNS servers. By default, this is \u0026ldquo;127.0.0.1\u0026rdquo; datacenter: controls the datacenter in which the agent is running. If not provided, it defaults to \u0026ldquo;dc1\u0026rdquo;. ui: Enables the built-in web UI server and the required HTTP routes. This eliminates the need to maintain the Consul web UI files separately from the binary. log-level: Log level of the agent bootstrap-expect: hints to the Consul server the number of additional server nodes we are expecting to join. enable-script-checks: controls whether health checks that execute scripts are enabled on this agent, and defaults to false so operators must opt-in to allowing these. ui: Enables the built-in web UI server and the required HTTP routes. This eliminates the need to maintain the Consul web UI files separately from the binary. encrypt: Specifies the secret key to use for encryption of Consul network traffic. This key must be 16-bytes that are Base64-encoded. key_file cert_file ca_file retry-join: allows retrying a join if the first attempt fails  consul agent -retry-join \u0026quot;provider=aws tag_key=... tag_value=...\u0026quot; consul agent -retry-join \u0026quot;provider=gce project_name=... tag_value=...\u0026quot; consul agent -retry-join \u0026quot;provider=aliyun region=... tag_key=consul tag_value=... access_key_id=... access_key_secret=...\u0026quot;  Create the Necessary Directory and System Structure  Create the user now by typing:  adduser consul   create the configuration hierarchy  mkdir -p /opt/consul   create a location where consul can store persistent data  mkdir /opt/consul/data chown consul:consul /opt/consul/data  Setting up consul server cluster  in production environment, server should be 3-5 create server1 + bootstrap + ui  consul keygen X4SYOinf2pTAcAHRhpj7dA== consul agent -server \\ -client=0.0.0.0 \\ -bind=0.0.0.0 \\ -bootstrap-expect=3 \\ -datacenter=gcp-asia-east \\ -node=node1 \\ -data-dir=\u0026quot;/opt/consul/data\u0026quot; \\ -config-dir=\u0026quot;/opt/consul/conf\u0026quot; \\ -enable-script-checks=true \\ -encrypt=\u0026quot;X4SYOinf2pTAcAHRhpj7dA==\u0026quot; or using config file mkdir /opt/consul/conf consul agent -server -config-dir=\u0026quot;/opt/consul/conf\u0026quot; #/opt/consul/conf/default.json { \u0026quot;server\u0026quot;: true, \u0026quot;client\u0026quot;: \u0026quot;0.0.0.0\u0026quot;, \u0026quot;bind_addr\u0026quot;: \u0026quot;0.0.0.0\u0026quot;, \u0026quot;ports\u0026quot;: { \u0026quot;https\u0026quot;: 8080 } \u0026quot;datacenter\u0026quot;: \u0026quot;gcp-asia-east\u0026quot;, \u0026quot;data_dir\u0026quot;: \u0026quot;/opt/consul/data\u0026quot;, \u0026quot;enable-script-checks\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;log_level\u0026quot;: \u0026quot;INFO\u0026quot;, \u0026quot;node_name\u0026quot;: \u0026quot;server1\u0026quot;, \u0026quot;bootstrap_expect\u0026quot;: 3, \u0026quot;encrypt\u0026quot;: \u0026quot;X4SYOinf2pTAcAHRhpj7dA==\u0026quot;, \u0026quot;key_file\u0026quot;: \u0026quot;/etc/pki/tls/private/my.key\u0026quot;, \u0026quot;cert_file\u0026quot;: \u0026quot;/etc/pki/tls/certs/my.crt\u0026quot;, \u0026quot;ca_file\u0026quot;: \u0026quot;/etc/pki/tls/certs/ca-bundle.crt\u0026quot; }   in server2 and server 3  consul agent -node=server2 -data-dir=\u0026quot;/opt/consul/data\u0026quot; -config-dir=\u0026quot;/opt/consul/conf\u0026quot; -enable-script-checks=true -retry-join consul agent -node=server3 -data-dir=\u0026quot;/opt/consul/data\u0026quot; -config-dir=\u0026quot;/opt/consul/conf\u0026quot; -enable-script-checks=true -retry-join  consul service register  create a location where consul can store persistent data  mkdir /opt/consul/data  Consul支持兩種服務註冊的方式\n Using Consul HTTP API\n Using config document, example as below\n  #/opt/consul/conf/default.json { \u0026quot;service\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;web\u0026quot;, \u0026quot;tags\u0026quot;: [\u0026quot;dev\u0026quot;], \u0026quot;address\u0026quot;: \u0026quot;192.168.113.175\u0026quot;, \u0026quot;port\u0026quot;: 80, \u0026quot;checks\u0026quot;: [ { \u0026quot;http\u0026quot;: \u0026quot;http://192.168.113.175/_health/\u0026quot;, \u0026quot;interval\u0026quot;: \u0026quot;10s\u0026quot; } ] } }   啟動 consul agent  consul agent -data-dir /opt/consul/data -node=web1 -bind=192.168.113.175 -config-dir=\u0026quot;/opt/consul/conf/\u0026quot; \u0026amp;   agnet 加入 server  consul join {{ SERVER_FQDN_OR_IP }}  service discovery  Defining a Service  echo '{\u0026quot;service\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;web\u0026quot;, \u0026quot;tags\u0026quot;: [\u0026quot;dev\u0026quot;], \u0026quot;port\u0026quot;: 80, \u0026quot;interval\u0026quot;: \u0026quot;10s\u0026quot;}}' | sudo tee ./conf/web.json   Querying Services  dig @127.0.0.1 -p 8600 web.service.consul ;; QUESTION SECTION: ;web.service.consul. IN A ;; ANSWER SECTION: web.service.consul. 0 IN A 192.168.113.175 dig @127.0.0.1 -p 8600 dev.web.service.consul ;; QUESTION SECTION: ;dev.web.service.consul. IN A ;; ANSWER SECTION: dev.web.service.consul. 0 IN A 192.168.113.175 dig @127.0.0.1 -p 8600 web.service.consul SRV ;; QUESTION SECTION: ;web.service.consul. IN SRV ;; ANSWER SECTION: web.service.consul. 0 IN SRV 1 1 80 web1.gcp-asia-east.consul. web.service.consul. 0 IN SRV 1 1 80 web2.gcp-asia-east.consul. ;; ADDITIONAL SECTION: web1.gcp-asia-east.consul. 0 IN A 192.168.113.175 web2.gcp-asia-east.consul. 0 IN A 192.168.113.176  "
},
{
	"uri": "https://blog.twasa.cf/post/powershell/",
	"title": "Powershell",
	"tags": ["Microsoft", "powershell"],
	"description": "",
	"content": " Powershell PowerShell Core is a cross-platform (Windows, Linux, and macOS) automation and configuration tool/framework that works well with your existing tools and is optimized for dealing with structured data (e.g. JSON, CSV, XML, etc.), REST APIs, and object models. It includes a command-line shell, an associated scripting language and a framework for processing cmdlets.\ncmdlet  get all cmdlet  Get-Command Get-Command -Name *IP* Get-Command -Module ISE -Name *IP* Get-Help Get-Process Get-Member Get-Process | Where-Object {$_.Name -eq \u0026quot;powershell\u0026quot;} #$_ is the current pipeline object   cmdlet combind example: find all file mode = a  Get-ChildItem $env:USERPROFILE -Recurse -Force | Where-Object {$_.Mode -like \u0026quot;*a*\u0026quot;}   Using  get powershell profile path\nGet-Variable profile | Format-List  set alias\nSet-Alias ll Get-ChildItem  define variable\n$sourcedir = \u0026quot;r:\\MYDATA\u0026quot; #string $tmp1 = $sourcedir -join '.rar ' $tmp2 = $tmp1 + \u0026quot;.rar\u0026quot; $rarfile = $tmp2 -split \u0026quot; \u0026quot; $targetpath = \u0026quot;e:\\\u0026quot;, \u0026quot;x:\\\u0026quot;, \u0026quot;f:\\\u0026quot;, \u0026quot;y:\\\u0026quot; #  get variable type\n$variable.GetType().FullName  Comparison Operators\n     Purpose Operator Example     Greater than -gt 1 -gt 2 (Returns $false)   Less than -lt 1 -lt 2 (Returns $true)   Equals -eq 2 -eq 1+1 (Returns $true)   Not equals -ne 3 -ne 1+1 (Returns $true)   Greater than or equal to -ge 3 -ge 1+1 (Returns $true)   Less than or equal to -le 2 -le 1+1 (Returns $true)     statements: if else\nif (Test-Path $item){ if ($? -notmatch \u0026quot;True\u0026quot;){ #do something } } Else { #do something }  statements: For loop\nFor ($i=0; $i -le 10; $i++) { \u0026quot;10 * $i = \u0026quot; + (10 * $i) }  statements: foreach\n$items = 1,2,3,4,5 ForEach ($item in $items){ #do something }  define function\n  function FunctionNAME { #do something } function FunctionNAME($parm1){ #do something } function FunctionNAME([String] $parm1){ #do something } function FunctionNAME([String] $parm1 = \u0026quot;hello\u0026quot;){ #do something return $parm1 }   call function\nFunctionNAME FunctionNAME \u0026quot;test\u0026quot; FunctionNAME -parm1 \u0026quot;echo\u0026quot;  prevent external Windows subsystem based EXE, for each command to end before starting the next\n  \u0026lt;path to exe\u0026gt; | Out-Null Start-Process \u0026lt;path to exe\u0026gt; -NoNewWindow -Wait #powershell 2.0 $job = Start-Job \u0026lt;path to exe\u0026gt; Wait-Job $job Receive-Job $job   get ClipboardText ``` add-type -an system.windows.forms  5.1 Get-Clipboard [-Format ] [-TextFormatType ] [-Raw] []\n - set ClipboardText  add-type -an system.windows.forms System.Windows.Forms.Clipboard::SetText(\u0026lsquo;hello world\u0026rsquo;)\n5.1 Set-Clipboard [-Append] [-Value]  -Path  -LiteralPath  [-AsHtml] [-WhatIf] [-Confirm] [] ```\n"
},
{
	"uri": "https://blog.twasa.cf/programing/tools/vscode-sync/",
	"title": "Vscode Sync",
	"tags": ["Development"],
	"description": "",
	"content": " Visual Studio Code Visual Studio Code is a code editor redefined and optimized for building and debugging modern web and cloud applications. basic Configuration  \u0026quot;editor.wordWrap\u0026quot;: \u0026quot;off\u0026quot;, \u0026quot;[markdown]\u0026quot;: { \u0026quot;editor.wordWrap\u0026quot;: \u0026quot;off\u0026quot;, \u0026quot;editor.quickSuggestions\u0026quot;: false }, \u0026quot;markdown.extension.preview.autoShowPreviewToSide\u0026quot;: true, \u0026quot;workbench.colorCustomizations\u0026quot;: { \u0026quot;tab.activeBackground\u0026quot;: \u0026quot;#0300aa\u0026quot; }  Extensions  Markdown All in One Settings Sync Copy Relative Path Path Intellisense Markdown table prettifier Microsoft Python extension  Settings sync using github  Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new token Give your token a descriptive name like vscode, check gist and click Generate token. Copy and backup your token. Type upload in VSCode Command Palette. Enter your GitHub Personal Access Token.  "
},
{
	"uri": "https://blog.twasa.cf/post/docker/",
	"title": "Docker Memo(drafting)",
	"tags": ["Docker"],
	"description": "",
	"content": " Docker Description  is an open platform for developers and sysadmins to build, ship, and run distributed applications, whether on laptops, data center VMs, or the cloud Containers vs. virtual machines  Virtual Machine diagram    Container diagram  docker concept  Image : a lightweight, stand-alone, executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, environment variables, and config files.\n Container : a runtime instance of an imageâ€”what the image becomes in memory when actually executed. It runs completely isolated from the host environment by default, only accessing host files and ports if configured to do so.\n registries : a place to store and distribute Docker images, like Docker Hub\n Repository : a collection of different versions for a single Docker image\n Dockerfile : a text document that contains all the commands a user could call on the command line to assemble an image. Using docker build users can create an automated build that executes several command-line instructions in succession.\n Docker Machine : a tool that lets you install Docker Engine on virtual hosts, and manage the hosts with docker-machine commands. You can use Machine to create Docker hosts on your local Mac or Windows box, on your company network, in your data center, or on cloud providers like Azure, AWS, or Digital Ocean.\n Compose : a tool for defining and running multi-container Docker applications\n Docker Swarm (1.12.0 or later) : enables you to create a cluster of one or more Docker Engines called a swarm. A swarm consists of one or more nodes: physical or virtual machines running Docker Engine 1.12 or later in swarm mode.   Requirement  64bit environment Linux kernel \u0026gt; 3.10  installation  reference https://docs.docker.com/engine/installation/ Ubuntu\natp-get update curl -fsSL https://get.docker.com/ | sh  CentOS 6\nyum install epel-release yum install docker-io  CentOS 7\nyum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum-config-manager --enable docker-ce-edge yum install docker-ce systemctl enable docker systemctl start docker   commands and syntax  Usage:  docker [OPTIONS] COMMAND [ARG...] docker [ --help | -v | --version ] A self-sufficient runtime for containers. Options: --config string Location of client config files (default \u0026quot;/root/.docker\u0026quot;) -D, --debug Enable debug mode --help Print usage -H, --host value Daemon socket(s) to connect to (default []) -l, --log-level string Set the logging level (\u0026quot;debug\u0026quot;|\u0026quot;info\u0026quot;|\u0026quot;warn\u0026quot;|\u0026quot;error\u0026quot;|\u0026quot;fatal\u0026quot;) (default \u0026quot;info\u0026quot;) --tls Use TLS; implied by --tlsverify --tlscacert string Trust certs signed only by this CA (default \u0026quot;/root/.docker/ca.pem\u0026quot;) --tlscert string Path to TLS certificate file (default \u0026quot;/root/.docker/cert.pem\u0026quot;) --tlskey string Path to TLS key file (default \u0026quot;/root/.docker/key.pem\u0026quot;) --tlsverify Use TLS and verify the remote -v, --version Print version information and quit   Commands     Command Description     docker attach Attach local standard input, output, and error streams to a running container   docker build Build an image from a Dockerfile   docker checkpoint Manage checkpoints   docker commit Create a new image from a container’s changes   docker config Manage Docker configs   docker container Manage containers   docker cp Copy files/folders between a container and the local filesystem   docker create Create a new container   docker deploy Deploy a new stack or update an existing stack   docker diff Inspect changes to files or directories on a container’s filesystem   docker events Get real time events from the server   docker exec Run a command in a running container   docker export Export a container’s filesystem as a tar archive   docker history Show the history of an image   docker image Manage images   docker images List images   docker import Import the contents from a tarball to create a filesystem image   docker info Display system-wide information   docker inspect Return low-level information on Docker objects   docker kill Kill one or more running containers   docker load Load an image from a tar archive or STDIN   docker login Log in to a Docker registry   docker logout Log out from a Docker registry   docker logs Fetch the logs of a container   docker network Manage networks   docker node Manage Swarm nodes   docker pause Pause all processes within one or more containers   docker plugin Manage plugins   docker port List port mappings or a specific mapping for the container   docker ps List containers   docker pull Pull an image or a repository from a registry   docker push Push an image or a repository to a registry   docker rename Rename a container   docker restart Restart one or more containers   docker rm Remove one or more containers   docker rmi Remove one or more images   docker run Run a command in a new container   docker save Save one or more images to a tar archive (streamed to STDOUT by default)   docker search Search the Docker Hub for images   docker secret Manage Docker secrets   docker service Manage services   docker stack Manage Docker stacks   docker start Start one or more stopped containers   docker stats Display a live stream of container(s) resource usage statistics   docker stop Stop one or more running containers   docker swarm Manage Swarm   docker system Manage Docker   docker tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE   docker top Display the running processes of a container   docker unpause Unpause all processes within one or more containers   docker update Update configuration of one or more containers   docker version Show the Docker version information   docker volume Manage volumes   docker wait Block until one or more containers stop, then print their exit codes    commands examples  docker run hello-world\ndocker run hello-world  check docker version\ndocker --version  check docker images\ndocker images   Using Dockerfile  Create your project directory Change directories (cd) into the directory create a file called Dockerfile, sample code as below  # Use an official Python runtime as a parent image FROM python:2.7-slim # Set the working directory to /app WORKDIR /app # Copy the current directory contents into the container at /app ADD . /app # Install any needed packages specified in requirements.txt RUN pip install -r requirements.txt # Make port 80 available to the world outside this container EXPOSE 80 # Define environment variable ENV NAME World # Run app.py when the container launches CMD [\u0026quot;python\u0026quot;, \u0026quot;app.py\u0026quot;]   Create a requirements.txt  Flask Redis   Create a app.py  from flask import Flask from redis import Redis, RedisError import os import socket # Connect to Redis redis = Redis(host=\u0026quot;redis\u0026quot;, db=0, socket_connect_timeout=2, socket_timeout=2) app = Flask(__name__) @app.route(\u0026quot;/\u0026quot;) def hello(): try: visits = redis.incr(\u0026quot;counter\u0026quot;) except RedisError: visits = \u0026quot;\u0026lt;i\u0026gt;cannot connect to Redis, counter disabled\u0026lt;/i\u0026gt;\u0026quot; html = \u0026quot;\u0026lt;h3\u0026gt;Hello {name}!\u0026lt;/h3\u0026gt;\u0026quot; \\ \u0026quot;\u0026lt;b\u0026gt;Hostname:\u0026lt;/b\u0026gt; {hostname}\u0026lt;br/\u0026gt;\u0026quot; \\ \u0026quot;\u0026lt;b\u0026gt;Visits:\u0026lt;/b\u0026gt; {visits}\u0026quot; return html.format(name=os.getenv(\u0026quot;NAME\u0026quot;, \u0026quot;world\u0026quot;), hostname=socket.gethostname(), visits=visits) if __name__ == \u0026quot;__main__\u0026quot;: app.run(host='0.0.0.0', port=80)   Build the app  docker build -t friendlyhello .   check the docker image  docker images REPOSITORY TAG IMAGE ID CREATED SIZE friendlyhello latest 2af32fdfad1d 9 seconds ago 195.4 MB docker.io/python 2.7-slim 8b88f06b72d7 8 days ago 183.6 MB docker.io/hello-world latest 05a3bd381fc2 2 weeks ago 1.84 kB   run the app  docker run -p 4000:80 friendlyhello   run the app in the background, in detached mode:  docker run -d -p 4000:80 friendlyhello   get running container  docker ps docker container ls #v1.3 or upper only   stop the running container  docker stop \u0026lt;CONTAINER ID\u0026gt;   Tag the image  docker tag image username/repository:tag docker tag friendlyhello william/get-started:part2   Remove one or more images  docker rmi \u0026lt;container id\u0026gt; docker image rm \u0026lt;image id\u0026gt;   Publish the image  docker push username/repository:tag   Pull and run the image from the remote repository  docker run -p 4000:80 username/repository:tag  Using docker-compose for docker stack deploy  create a docker-compose.yml tells Docker to do the following: Pull the image we uploaded in step 2 from the registry. Run 2 instances of that image as a service called web, limiting each one to use, at most, 10% of the CPU (across all cores), and 50MB of RAM. Immediately restart containers if one fails. Map port 4000 on the host to web’s port 80. Instruct web’s containers to share port 80 via a load-balanced network called webnet. (Internally, the containers themselves will publish to web’s port 80 at an ephemeral port.) Define the webnet network with the default settings (which is a load-balanced overlay network).  version: \u0026quot;3\u0026quot; services: web: # replace username/repo:tag with your name and image details image: william/get-started:part2 deploy: replicas: 2 resources: limits: cpus: \u0026quot;0.1\u0026quot; memory: 50M restart_policy: condition: on-failure ports: - \u0026quot;4000:80\u0026quot; networks: - webnet networks: webnet:   for load-balance, run swarm init first\ndocker swarm init  run docker stack\ndocker stack deploy -c docker-compose.yml getstartedlab  Get the service ID for the one service in our application:\ndocker service ls  List the tasks\ndocker service ps \u0026lt;service\u0026gt;  check task information\ndocker inspect --format='{{.Status.ContainerStatus.ContainerID}}' \u0026lt;task or containerid\u0026gt;  Scale the app by changing the replicas value in docker-compose.yml, saving the change, and re-running the docker stack deploy command:\ndocker stack deploy -c docker-compose.yml getstartedlab  Take down the app and the swarm\ndocker stack rm getstartedlab docker swarm leave --force   install docker-machine  install\ncurl -L https://github.com/docker/machine/releases/download/v0.12.2/docker-machine-`uname -s`-`uname -m` \u0026gt;/tmp/docker-machine \u0026amp;\u0026amp; chmod +x /tmp/docker-machine \u0026amp;\u0026amp; sudo cp /tmp/docker-machine /usr/local/bin/docker-machine  check version\ndocker-machine version  uninstall\nrm $(which docker-machine)   Swarm clusters  TCP port 2376 for secure Docker client communication. This port is required for Docker Machine to work. Docker Machine is used to orchestrate Docker hosts.\n TCP port 2377. This port is used for communication between the nodes of a Docker Swarm or cluster. It only needs to be opened on manager nodes.\n TCP and UDP port 7946 for communication among nodes (container network discovery).\n UDP port 4789 for overlay network traffic (container ingress networking).\n Manager :\n Worker(Node) :\n  "
},
{
	"uri": "https://blog.twasa.cf/programing/python/python-timezone-convert/",
	"title": "Python Timezone Convert example code",
	"tags": ["Development"],
	"description": "",
	"content": " Requirement  Python python-dateutil  example code from datetime import datetime from dateutil import tz # METHOD Auto-detect zones: #from_zone = tz.tzutc() #to_zone = tz.tzlocal() from_zone = raw_input(\u0026quot;From Timezone: \u0026quot;) to_zone = raw_input(\u0026quot;To Timezone: \u0026quot;) from_zone = tz.gettz(from_zone) to_zone = tz.gettz(to_zone) def datainput(): # utc = datetime.utcnow() year = raw_input(\u0026quot;Year (A.D.): \u0026quot;) month = raw_input(\u0026quot;Month: \u0026quot;) day = raw_input(\u0026quot;day: \u0026quot;) hour = raw_input(\u0026quot;hour: \u0026quot;) minute = raw_input(\u0026quot;minute: \u0026quot;) datetimestring = year + '-' + month + '-' + day + ' ' + hour + ':' + minute + ':00' return datetimestring dtstr = datainput() utc = datetime.strptime(dtstr, '%Y-%m-%d %H:%M:%S') # Tell the datetime object that it's in UTC time zone since # datetime objects are 'naive' by default utc = utc.replace(tzinfo=from_zone) # Convert time zone central = utc.astimezone(to_zone) print central.isoformat()  "
},
{
	"uri": "https://blog.twasa.cf/linux/openssl/",
	"title": "openssl",
	"tags": ["commands"],
	"description": "",
	"content": " create private key openssl genrsa -out server.key 2048  create Certificate Signing Request openssl req -sha512 -new -key server.key -out server.csr -subj \u0026quot;/C=TW/ST=Taipei/L=Taipei/O=example/OU=Personal/CN=www.example.com\u0026quot;  check csr info openssl req -in server.csr -noout -text  Self-Sign Certificate openssl x509 -sha512 -req -days 3650 -in server.csr -signkey server.key -out server.crt  encrypt private key -a 表示檔案使用 base64 編碼 -salt 加雜湊值，增加安全性 openssl aes-256-cbc -a -salt -in server.key -out server.encrypt.key  "
},
{
	"uri": "https://blog.twasa.cf/linux/logrotation/",
	"title": "Linux Logrotation",
	"tags": ["Logrotation"],
	"description": "",
	"content": " config options rotate count  rotate n  permission  create 0664 USER GROUP  by interval  daily weekly monthly yearly  by size  size 100k size 100M size 100G  archive  compress nocompress  Postrotate postrotate /usr/sbin/apachectl restart \u0026gt; /dev/null endscript\nscript  prerotate：executed before the log file is rotated postrotate：在做完 logrotate 之後啟動的指令，例如重新啟動 (kill -1 或 kill -HUP) 某個服務;  postrotate /bin/kill -HUP `cat /var/run/syslogd.pid 2\u0026gt; /dev/null` 2\u0026gt; /dev/null || true endscript }   others  missingok：If the log file is missing, go on to the next one without issu-ing an error message  "
},
{
	"uri": "https://blog.twasa.cf/post/jupyterhub/",
	"title": "Jupyterhub",
	"tags": ["Development"],
	"description": "",
	"content": " JupyterHub on CentOS 7 requirement  a Linux/Unix based system Python 3.4 or greater wget pip npm  install  yum search python3 yum -y install python3X yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel npm npm install -g configurable-http-proxy wget https://bootstrap.pypa.io/get-pip.py python3.X get-pip.py yum -y install python-devel python3X-devel pip3 install jupyterhub ipython[notebook]  startup  jupyterhub \u0026ndash;ip 0.0.0.0 \u0026ndash;port 8443 \u0026ndash;ssl-key my_ssl.key \u0026ndash;ssl-cert my_ssl.cert jupyterhub -f /path/to/jupyterhub_config.py  config  jupyterhub \u0026ndash;generate-config vi jupyterhub_config.py  c.JupyterHub.ip = '0.0.0.0' c.JupyterHub.port = 8443 c.JupyterHub.ssl_key = '/path/to/ssl.key' c.JupyterHub.ssl_cert = '/path/to/ssl.cert' c.Spawner.notebook_dir= '~/'   systemd  vi /usr/lib/systemd/system/jupyterhub.service  [Unit] Description=Jupyter Notebook After=network.target [Service] Type=simple User=root Group=root Restart=always RestartSec=10 WorkingDirectory=/etc/nagios/ ExecStart=/usr/bin/jupyterhub -f /root/jupyterhub_config.py [Install] WantedBy=multi-user.target   systemctl reload jupyterhub systemctl enable jupyterhub  "
},
{
	"uri": "https://blog.twasa.cf/post/git/",
	"title": "Git",
	"tags": ["Development"],
	"description": "",
	"content": " Git的基本使用 Config  在每一次的 Git commit (提交，我們稍後會提到) 都會記錄作者的訊息像是 name 及 email，因此我們使用下面的指令來設定：\ngit config --global user.name \u0026quot;你的姓名\u0026quot; git config --global user.email \u0026quot;你的@email位址\u0026quot;  加上 \u0026ndash;global 表示是全域的設定。你可以使用 git config \u0026ndash;list 這個指令來看你的 Git 設定內容\n Git 也有提供 alias 的功能，例如你可以將 git status 縮寫為 git st，git checkout 縮寫為 git co 等，你只要這樣設定，這樣一來只要打 git st 就等同於打 git status\ngit config --global alias.st status git config --global alias.ck checkout git config --global alias.rst reset HEAD  忽略空白\ngit config --global apply.whitespace nowarn  Git 預設輸出是沒有顏色的，我們可以讓他在輸出時加上顏色讓我們更容易閱讀\ngit config --global color.ui true  設定commit的預設編輯器\ngit config --global core.editor vim  設定merge的比對工具\ngit config --global merge.tool vimdiff  不希望加入版本控制的追蹤設定: 建立.gitignore檔案，內容\n*.swp log/*.log   github ssh-key config  create a ssh key\nssh-keygen -t rsa -C \u0026quot;your_email@youremail.com\u0026quot;  upload public key to github\nlogin to GitHub Account Settings -\u0026gt; SSH Public Keys -\u0026gt; Add another public key cat id_rsa_github.pub # past your public key content  vi .ssh/config\nIdentityfile ~/.ssh/your-private-key   基本指令  建立一個新的 Repository\ngit init  Clone(複製)別人的 Repository：\ngit clone https://xxx@github.com/xxx/xxx.git or git glone git://xxx@github.com/xxx/xxx.git  檢查目前 Git 的狀態\ngit status  加檔案\ngit add git add -p #批次加入，可用在開發時有些程式碼功能想加入，有些不想加入時使用  移檔案:\ngit revert  提交檔案:\ngit commit or git commit -m \u0026quot;說明文字\u0026quot;  加檔案並提交\ngit commit -am \u0026quot;Add test.py to test git function\u0026quot;  查看過去 commit 的紀錄 ``` git log or git log \u0026ndash;stat or git log -p\n  git reflog\n - About HEAD  在Git中，HEAD像是一個指標，指著一個版本 HEAD^ 把指標只到上一個版本 HEAD^^ 把指標只到上一個版本 HEAD~100 把指標只到上100個版本\n - 回之前版本  git reset \u0026ndash;hard HEAD^\ngit reset \u0026ndash;hard \n - 放棄修改(git add之前)  git checkout \u0026ndash; FILE\n - 放棄修改(git add之後)  git reset HEAD FILE git checkout \u0026ndash; FILE\n - 放棄修改(git commit之後，尚未push至remote之前都有救)  git checkout  \n - 列出既有標籤:  git tag -l\n - 新增標籤 -a 就是標籤名稱，-m 代表該標籤說明  git tag -a v1.4 -m \u0026lsquo;my version 1.4\u0026rsquo;\n - 上傳標籤到遠端, git push並不會把標籤上傳到遠端，所以必須透過底下才行  git push origin v1.5\n - 如果在本機端很多標籤，利用 –tags 一次上傳上去  git push origin \u0026ndash;tags ```\n"
},
{
	"uri": "https://blog.twasa.cf/db/mongodb/",
	"title": "Mongodb basic",
	"tags": ["Mongodb"],
	"description": "",
	"content": " install  Create a /etc/yum.repos.d/mongodb-org-3.4.repo  [mongodb-org-3.4] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.4/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-3.4.asc   yum  yum install -y mongodb-org  config  default path /etc/mongod.\n# security db.createUser( { user: \u0026quot;帳號\u0026quot;, pwd: \u0026quot;密碼\u0026quot;, roles: [ { role: \u0026quot;userAdminAnyDatabase\u0026quot;, db: \u0026quot;admin\u0026quot; } ] } ) security.authorization : enabled   selinux semanage port -a -t mongod_port_t -p tcp 27017  shell connection mongo --port 27017 -u \u0026quot;unitadmin\u0026quot; -p \u0026quot;xyz123\u0026quot; --authenticationDatabase \u0026quot;test\u0026quot;  list current using db db  列出資料庫清單 show dbs  切換資料庫 use 資料庫名稱  驗證 db.auth('account', 'password')  顯示協助訊息 help  列出資料表 show collections  列出collection 資料筆數 db.products.count()  CRUD example: db.collectionname.find( {name: \u0026quot;william\u0026quot;}, #query {Name:1, address:1} #projection ).limit(5) #modifier  mongo  \u0026ndash;eval \u0026ldquo;db.dropDatabase()\u0026rdquo; 移除 database。 或者下面這種方式： \u0026gt; use mydb; \u0026gt; db.dropDatabase();  離開mongo Shell exit  直接執行 mongo 資料庫名稱 --eval \u0026quot;語法\u0026quot; mongo test --eval \u0026quot;db.egame.findOne()\u0026quot; mongo test --eval 'db.egame.remove({\u0026quot;createDateTime\u0026quot; : {$lt : ISODate(\u0026quot;2017-06-10T13:56:00.001Z\u0026quot;)}})'  使用js file帶入語法 mongo \u0026lt; script.js //script.js 內容 db.mycollection.findOne() db.getCollectionNames().forEach(function(collection) { print(collection); });  account and permission show accounts show users  create account for db backup use 要備份的資料庫名稱 db.createUser( { user: \u0026quot;帳號\u0026quot;, pwd: \u0026quot;密碼\u0026quot;, roles: [ { role: \u0026quot;backup\u0026quot;, db: \u0026quot;admin\u0026quot; } ] } )  for db only account use 資料庫名稱 db.createUser({ user: \u0026quot;帳號\u0026quot;, pwd: \u0026quot;密碼\u0026quot;, roles: [ { role: \u0026quot;userAdmin\u0026quot;, db: \u0026quot;資料庫名稱\u0026quot; }, { role: \u0026quot;readWrite\u0026quot;, db: \u0026quot;資料庫名稱\u0026quot; }, { role: \u0026quot;dbAdmin\u0026quot;, db: \u0026quot;資料庫名稱\u0026quot; } ] }); db.createUser({ user: \u0026quot;帳號\u0026quot;, pwd: \u0026quot;密碼\u0026quot;, roles: [ { role: \u0026quot;readWrite\u0026quot;, db: \u0026quot;資料庫名稱\u0026quot; }, ] }); db.updateUser( \u0026quot;betadmin\u0026quot;, { roles : [ { role: \u0026quot;userAdmin\u0026quot;, db: \u0026quot;資料庫名稱\u0026quot; }, { role: \u0026quot;readWrite\u0026quot;, db: \u0026quot;資料庫名稱\u0026quot; }, { role: \u0026quot;dbAdmin\u0026quot;, db: \u0026quot;資料庫名稱\u0026quot; } ], pwd: \u0026quot;密碼\u0026quot; } ) db.removeUser(帳號)  enable on config file security: authorization: enabled # 前面要空白,不然服務會無法啟動  compare with relational database    MongoDB MySQL     collections tables   documents rows    backup and restore  backup  mongodump --gzip --archive=backup-file-name --db db-name --collection collection-name -q \u0026quot;\u0026quot; mongodump -d 資料庫名稱 -c egame -q '{\u0026quot;createDateTime\u0026quot;:{$gte:ISODate(\u0026quot;2017-06-11T00:00:00.000Z\u0026quot;)}}' --gzip --archive=/tmp/backup.tgz --out filename mongodump -u$mongo_user -p$mongo_secret \\ -d $db -c $collection \\ --queryFile $DIR/query.json \\ --gzip --archive=\u0026quot;/tmp/${collection}-backup-${dumpdate}.tgz\u0026quot; query.json 內容 {\u0026quot;createDateTime\u0026quot;:{$gte:ISODate(\u0026quot;YYYY-MM-DDT00:00:00.000Z\u0026quot;)}}   restore  mongorestore --db db-name --collection collection-name filename mongorestore --archive=filename --gzip  Roles  refernece https://docs.mongodb.com/manual/reference/built-in-roles/  read：允許帳號讀取指定資料庫 readWrite：允許帳號讀寫指定資料庫 backup,retore:允許備份、還原，在db.createUser()方法中roles里面的db必須是admin資料庫，不然會錯誤 dbAdmin：允許帳號在指定資料庫中執行管理函數，如索引建立、删除，查看統計或訪問system.profile userAdmin：允許帳號向system.users這個collection寫入，可以找指定資料庫裡建立、刪除與管理帳號 clusterAdmin：只在admin資料庫中可用，赋予帳號所有分片和复制集相关函数的管理權限。 readAnyDatabase：只在admin資料庫中可用，赋予用户所有資料庫的讀取權限 readWriteAnyDatabase：只在admin資料庫中可用，赋予用户所有資料庫的读写權限 userAdminAnyDatabase：只在admin資料庫中可用，赋予用户所有資料庫的userAdmin權限， dbAdminAnyDatabase：只在admin資料庫中可用，赋予用户所有資料庫的dbAdmin權限。 root：只在admin資料庫中可用。等於最高權限帳號  remove yum erase $(rpm -qa | grep mongodb-org) rm -r /var/log/mongodb rm -r /var/lib/mongo  "
},
{
	"uri": "https://blog.twasa.cf/post/aws-cli/",
	"title": "Aws Cli",
	"tags": ["AWS"],
	"description": "",
	"content": " AWS CLI referneces syntax aws [options] \\\u0026lt;command\u0026gt; \\\u0026lt;subcommand\u0026gt; [\\\u0026lt;subcommand\u0026gt; ...] [parameters] options: --region --profile --filters \u0026quot;Name=instance-state-name,Values=running\u0026quot; --query  create or modify profile aws configure --profile PROFILE aws configure set default.s3.signature_version s3v4 aws configure set profile.your_profile_name.s3.signature_version s3v4  create key pairs aws --profile PROFILE --region REGION ec2 create-key-pair --key-name KEYNAME  create IAM users and permission aws --profile PROFILE iam create-user --user-name USERNAME aws --profile PROFILE iam create-access-key --user-name USERNAME aws --profile PROFILE iam put-user-policy --user-name USERNAME --policy-name POLICYNAME --policy-document file://POLICYDOCUMENT aws --profile PROFILE iam get-server-certificate --server-certificate-name CERTNAME XXXX  SES, SNS - aws --profile PROFILE --region REGION ses verify-email-identity --email-address YOUR@MAIL.ADDR - aws --profile PROFILE --region REGION sns create-topic --name TOPICNAME - aws --profile PROFILE --region REGION ses set-identity-notification-topic --identity YOUR@MAIL.ADDR --notification-type [Bounce, Complaint, Delivery] --sns-topic arn:aws:sns:us-east-1:EXAMPLE65304:MyTopic - aws --profile PROFILE --region REGION sns subscribe --topic-arn arn:aws:sns:us-east-1:EXAMPLE65304:MyTopic --protocol email --notification-endpoint YOUR@MAIL.ADDR  DynamoDB aws --profile PROFILE --region REGION s3api dynamodb create-table --table-name TABLENAME --cli-input-json file://JSONFILE  Create VPC, Subnet, internet gateway, route-table, security-group, and associate - aws ec2 --profile PROFILE--region REGION create-vpc --cidr-block 10.10.0.0/16 - aws ec2 --profile PROFILE--region REGION create-tags â€“resources vpc-xxxx -tags Key=Name,Value=$ENV_$LOCATION_VPC01 - aws ec2 --profile PROFILE--region REGION modify-vpc-attribute --vpc-id vpc-xxxx --enable-dns-hostnames - aws ec2 --profile PROFILE--region REGION create-subnet --vpc-id vpc-xxxx --cidr-block 10.10.1.0/24 - aws ec2 --profile PROFILE--region REGION create-tags â€“resources subnet-xxxx â€“tags Key=Name,Value=$ENV_$LOCATION_VPC01_10.10.1.0 - aws ec2 --profile PROFILE--region REGION create-internet-gateway - aws ec2 --profile PROFILE--region REGION create-tags -resources igw-xxxx -tag Key=Name,Value=$ENV_$LOCATION_VPC01_GW01 - aws ec2 --profile PROFILE--region REGION attach-internet-gateway --internet-gateway-id igw-xxxx --vpc-id vpc-xxxx - aws ec2 --profile PROFILE--region REGION create-route-table â€“vpc-id vpc-xxxx - aws ec2 --profile PROFILE--region REGION create-route --route-table-id rtb-xxxx --destination-cidr-block 0.0.0.0/0 --gateway-id igw-xxxx - aws ec2 --profile PROFILE--region REGION associate-route-table --route-table-id rtb-xxxx --subnet-id subnet-xxxx - aws ec2 --profile PROFILE--region REGION create-security-group --group-name $ENV_$LOCATION_VPC01_SG01 --description $ENV_$LOCATION_VPC01_SG01 - aws ec2 --profile PROFILE--region REGION authorize-security-group-ingress --group-name MySecurityGroup --protocol tcp --port 22 --cidr 203.0.113.0/24 - aws ec2 --profile PROFILE--region REGION run-instances --image-id ami-xxxx --count 1 --instance-type m4.xlarge --key-name MyKeyPair --security-group-ids sg-xxxx --subnet-id subnet-xxxx - aws ec2 --profile PROFILE--region REGION allocate-address - aws ec2 --profile PROFILE--region REGION describe-instances --instance-id i-xxx --query 'Reservations[\\*].Instances[\\*].NetworkInterfaces[*].NetworkInterfaceId' --output text - aws --profile mp ec2 describe-instances --region us-west-2 --filters \u0026quot;Name=instance-state-name,Values=running\u0026quot; --query \u0026quot;Reservations[*].Instances[*].[Tags[?Key=='Name'].Value,PublicIpAddress]\u0026quot; --output text - aws ec2 --profile PROFILE--region REGION associate-address --instance-id i-xxx --public-ip xxxx --network-interface-id eni-xxxx  S3 list buckets aws --profile PROFILE --region REGION s3api list-buckets  create bucket aws --profile PROFILE s3api create-bucket --acl private --bucket qa-fw-ead98f12 --region REGION --create-bucket-configuration LocationConstraint=ap-northeast-1  set bucket acl aws --profile PROFILE --region REGION s3api put-bucket-acl --bucket qa-fw-ead98f12 --access-control-policy file://D:\\workspace\\Your\\jsons\\file.json  enable bucket log aws --profile PROFILE --region REGION s3api put-bucket-logging --bucket qa-fw-ead98f12 --bucket-logging-status file://D:\\workspace\\Your\\jsons\\file.json  set bucket lifecycle aws --profile PROFILE --region REGION s3api put-bucket-lifecycle-configuration --bucket qa-fw-ead98f12 --lifecycle-configuration file://D:\\workspace\\Your\\jsons\\file.json  create folders aws --profile PROFILE --region eu-west-1 s3api put-object --bucket mp-eu-ead98f12 --key [8e15, dfa9, e94d, 487f, event]/  upload file to bucket aws --profile PROFILE --region REGION s3 cp FILEPATH/FILENAME s3://BUCKETNAME/FILENAME --acl public-read  bucket policy for elb access logs, and IAM users  rererence http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-access-logs.html   { \u0026quot;Id\u0026quot;: \u0026quot;Policy1429136655940\u0026quot;, \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;Stmt1429136633762\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:PutObject\u0026quot; ], \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::BUCKETNAME/FOLDERNAME/AWSLogs/AWSACCOUNTID_OF_ELB/*\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: [ \u0026quot;ELBACCOUNTID\u0026quot; ] } } ] }  for Cross-Account Access  reference http://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html  aws account A ID number 111111111111 (resources owner) create a IAM role, Role Type = Role for Cross-Account Access, Provide access between AWS accounts you own, input another account ID, and Attach Policy, final copy Role ARN aws account B ID number 222222222222 (resources accessor) create a IAM Custom Policy for allow { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::111111111111:role/RoleARN\u0026quot; } } create a IAM Custom Policy for deny { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: { \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::111111111111:role/RoleARN\u0026quot; } } #Apply Allow Policy to resources access group, and apply Deny Policy to non-resources access group  ELB(Classic Load Balancer) create aws elb create-load-balancer --load-balancer-name my-load-balancer --listeners \u0026quot;Protocol=HTTP,LoadBalancerPort=80,InstanceProtocol=HTTP,InstancePort=80\u0026quot; --availability-zones us-west-2a us-west-2b aws elb create-load-balancer --load-balancer-name my-load-balancer --listeners \u0026quot;Protocol=HTTP,LoadBalancerPort=80,InstanceProtocol=HTTP,InstancePort=80\u0026quot; \u0026quot;Protocol=HTTPS,LoadBalancerPort=443,InstanceProtocol=HTTP,InstancePort=80,SSLCertificateId=arn:aws:iam::123456789012:server-certificate/my-server-cert\u0026quot; --availability-zones us-west-2a us-west-2b  modify attribute aws elb modify-load-balancer-attributes --load-balancer-name XXXX --load-balancer-attributes CrossZoneLoadBalancing={Enabled=boolean},AccessLog={Enabled=boolean,S3BucketName=string,EmitInterval=integer,S3BucketPrefix=string},ConnectionDraining={Enabled=boolean,Timeout=integer},ConnectionSettings={IdleTimeout=integer},AdditionalAttributes=[{Key=string,Value=string},{Key=string,Value=string}]  RDS create snapshot aws --profile PROFILE --region REGION rds delete-db-instance --db-instance-identifier mp-op-rds --no-skip-final-snapshot --final-db-snapshot-identifier mp-op-rds-final-snapshot  restore from snapshot aws --profile PROFILE --region REGION rds restore-db-instance-from-db-snapshot --db-instance-identifier mp-op-rds --db-snapshot-identifier mp-op-rds-final-snapshot --db-instance-class db.m3.large --db-subnet-group-name mp-op-rds-subg --no-multi-az --no-publicly-accessible --no-auto-minor-version-upgrade  wait rds available aws --profile PROFILE --region REGION rds wait db-instance-available --db-instance-identifier mp-op-rds  change parameter group aws --profile PROFILE --region REGION rds modify-db-instance --db-instance-identifier RDSNAME --db-parameter-group-name PGNAME --vpc-security-group-ids SGID --apply-immediately)  modify parameter group - aws --profile PROFILE --region REGION rds modify-db-parameter-group --db-parameter-group-name PGNAME --cli-input-json  Route53 list hosted zones aws --profile PROFILE --region REGION route53 list-hosted-zones  get hosted zone info aws --profile PROFILE --region REGION route53 get-hosted-zone --id \u0026quot;xxxxxxxxxxxx\u0026quot;  list record sets of hosted zone aws --profile PROFILE --region REGION route53 list-resource-record-sets --hosted-zone-id \u0026quot;xxxxxxxxxxxx\u0026quot; aws --profile mp --region REGION route53 list-resource-record-sets --hosted-zone-id Z3FV870FH3DCS4 \u0026gt; \u0026quot;D:\\workspace\\MP\\aws_r53_auto_before_dcd_rcd_modify.json\u0026quot; aws --profile mp --region REGION route53 list-resource-record-sets --hosted-zone-id Z1L8DNQYY69L2Z \u0026gt; \u0026quot;D:\\workspace\\MP\\aws_r53_local_before_dcd_rcd_modify.json\u0026quot;  change resource record sets aws --profile PROFILE --region REGION route53 change-resource-record-sets --hosted-zone-id \u0026quot;xxxxxxxxxxxx\u0026quot; --change-batch file://C:\\awscli\\route53\\change-resource-record-sets.json JSON Syntax: { \u0026quot;Comment\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;Changes\u0026quot;: [ { \u0026quot;Action\u0026quot;: \u0026quot;CREATE\u0026quot;|\u0026quot;DELETE\u0026quot;|\u0026quot;UPSERT\u0026quot;, \u0026quot;ResourceRecordSet\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;SOA\u0026quot;|\u0026quot;A\u0026quot;|\u0026quot;TXT\u0026quot;|\u0026quot;NS\u0026quot;|\u0026quot;CNAME\u0026quot;|\u0026quot;MX\u0026quot;|\u0026quot;NAPTR\u0026quot;|\u0026quot;PTR\u0026quot;|\u0026quot;SRV\u0026quot;|\u0026quot;SPF\u0026quot;|\u0026quot;AAAA\u0026quot;, \u0026quot;SetIdentifier\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;Weight\u0026quot;: long, \u0026quot;Region\u0026quot;: \u0026quot;us-east-1\u0026quot;|\u0026quot;us-east-2\u0026quot;|\u0026quot;us-west-1\u0026quot;|\u0026quot;us-west-2\u0026quot;|\u0026quot;ca-central-1\u0026quot;|\u0026quot;eu-west-1\u0026quot;|\u0026quot;eu-west-2\u0026quot;|\u0026quot;eu-central-1\u0026quot;|\u0026quot;ap-southeast-1\u0026quot;|\u0026quot;ap-southeast-2\u0026quot;|\u0026quot;ap-northeast-1\u0026quot;|\u0026quot;ap-northeast-2\u0026quot;|\u0026quot;sa-east-1\u0026quot;|\u0026quot;cn-north-1\u0026quot;|\u0026quot;ap-south-1\u0026quot;, \u0026quot;GeoLocation\u0026quot;: { \u0026quot;ContinentCode\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;CountryCode\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;SubdivisionCode\u0026quot;: \u0026quot;string\u0026quot; }, \u0026quot;Failover\u0026quot;: \u0026quot;PRIMARY\u0026quot;|\u0026quot;SECONDARY\u0026quot;, \u0026quot;TTL\u0026quot;: long, \u0026quot;ResourceRecords\u0026quot;: [ { \u0026quot;Value\u0026quot;: \u0026quot;string\u0026quot; } ... ], \u0026quot;AliasTarget\u0026quot;: { \u0026quot;HostedZoneId\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;DNSName\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;EvaluateTargetHealth\u0026quot;: true|false }, \u0026quot;HealthCheckId\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;TrafficPolicyInstanceId\u0026quot;: \u0026quot;string\u0026quot; } } ... ] }  CloudWatch list-metrics aws cloudwatch list-metrics --namespace \u0026quot;AWS/ELB\u0026quot; #AWS Namespaces http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/aws-namespaces.html  get-metrics-statistics aws cloudwatch get-metric-statistics --metric-name CPUUtilization --start-time 2014-04-08T23:18:00 --end-time 2014-04-09T23:18:00 --period 3600 --namespace AWS/EC2 --statistics Maximum --dimensions Name=InstanceId,Value=i-abcdef  decode-authorization-message aws sts decode-authorization-message \u0026quot;MESSAGES\u0026quot;  "
},
{
	"uri": "https://blog.twasa.cf/post/apache-http-advanced/",
	"title": "Apache Http Advanced",
	"tags": ["Apache"],
	"description": "",
	"content": " http rewrite  syntax  \u0026lt;IfModule mod_rewrite.c\u0026gt; RewriteEngine On #啟用 RewriteBase /路徑 RewriteCond 變數 開關 RewriteCond 變數 條件 [選項] RewriteRule 規則 [選項] \u0026lt;/IfModule\u0026gt; # 開關 on, off # 選項 NC (no case) 不區分大小寫 F (force URL to be forbidden) 禁用URL,返回403HTTP狀態碼。 R[=code] (force redirect) 強制外部重定向 code 預設302, 最好使用301 G (force URL to be gone) 強制URL為GONE，返回410HTTP狀態碼 P (force proxy) 強制使用代理轉發。 N (next round) 重新從第一條規則開始運行重寫過程。 C (chained with next rule) 與下一條規則關聯 AND OR #if ( (A OR B) AND (C OR D) ) RewriteCond A [or] RewriteCond B [or] RewriteCond C RewriteCond D L (last rule) 表明當前規則是最後一條規則，停止分析以後規則的重寫 # 變數 - %{HTTP_HOST} !^www.example.com #聲明Client請求的主機中首碼不是 www.example.com - %{HTTP_HOST} !^8.8.8.8 #聲明Client請求的主機中首碼不是 8.8.8.8 - %{HTTP_HOST} !^$ #聲明Client請求的主機中首碼不為空 - %{REQUEST_FILENAME} - %{HTTP_USER_AGENT} - 條件 \u0026quot;android|blackberry|googlebot-mobile|iemobile|ipad|iphone|ipod|opera mobile|palmos|webos\u0026quot; #手機瀏覽器 - %{REQUEST_URI} !^user.php$ - %{SERVER_PORT} ^443$ - %{HTTP_REFERER} example.com #從別的網站連過來 RewriteCond %{HTTP_HOST} !=\u0026quot;\u0026quot; RewriteCond %{HTTP_HOST} . RewriteCond %{HTTP_HOST} !^$ RewriteCond {REQUEST_URI} !=/foo/bar RewriteCond %{REQUEST_URI} !^/foo/bar$ RewriteCond {SERVER_PORT} =443 RewriteCond %{SERVER_PORT} ^443$  en.example.com to www.example.com RewriteEngine on RewriteCond %{HTTP_HOST} ^en.example.com [NC] RewriteRule ^(.*) HTTP://www.example.com/ [L]  How To Redirect Users To Mobile Or Normal Web Site Based On Device Using mod_rewrite RewriteEngine On RewriteCond %{HTTP_USER_AGENT} \u0026quot;android|blackberry|googlebot-mobile|iemobile|ipad|iphone|ipod|opera mobile|palmos|webos\u0026quot; [NC] RewriteRule ^$ http://m.example.com/ [L,R=302] RewriteEngine On RewriteCond %{HTTPS} off RewriteRule (.*) https://%{HTTP_HOST}:443%{REQUEST_URI}  Rewrite http to https RewriteEngine On RewriteCond %{HTTP:X-Forwarded-Proto} =http RewriteRule . https://%{HTTP:Host}%{REQUEST_URI} [L,R=permanent]  Rewrite http to https using http 301 RewriteEngine On RewriteCond %{HTTP:X-Forwarded-Proto} ^http$ RewriteRule .* https://%{HTTP_HOST}%{REQUEST_URI} [R=301,L]  Rewrite http to https by port RewriteEngine On RewriteCond %{SERVER_PORT} !443 RewriteRule ^(/(.*))?$ https://%{HTTP_HOST}/$1 [R=301,L]  Rewrite by detect url and user-agent  RewriteEngine On RewriteCond %{HTTP_USER_AGENT} \u0026quot;android|blackberry|googlebot-mobile|iemobile|ipad|iphone|ipod|operamobile|palmos|webos\u0026quot; [AND,NC] RewriteCond %{}% RewriteCond RewriteRule ^$ http://m.example.com/ [L,R=301]  Reverse Proxy ProxyRequests off ProxyPreserveHost on RequestHeader set X-Forwarded-Proto https RequestHeader set X-Forwarded-Port 443 ProxyPass / http://127.0.0.1:8080/ ProxyPassReverse / http://127.0.0.1:8080/  Reverse Proxy with VirtualHost ProxyRequests off \u0026lt;VirtualHost *:80\u0026gt; ProxyVia On ProxyPreserveHost On RequestHeader set Host \u0026quot;t9admin.yolo168.xyz\u0026quot; ServerName t9admin.yolo168.xyz #ProxyPass / http://t9admin.04670467.com/ #ProxyPassReverse / http://t9admin.04670467.com/ ProxyPass / http://54.64.75.75/ ProxyPassReverse / http://54.64.75.75/ \u0026lt;/VirtualHost\u0026gt; 關於 ProxyPassReverse 如果今天你的ProxyPass的設定為 ProxyPass /forum http://172.20.100.200/forum 可能會出現問題，因為rp會幫你把網址慮掉，也就是說 今天我 http://172.20.100.200/forum 事實上是轉到 http://172.20.100.220/forum 不過當我連 http://172.20.100.200/forum/blog 時，卻是轉到 http://172.20.100.220/blog 當網頁資料夾還有子目錄時，rp會自動蓋掉相同的path 所以要加上ProxyPassReverse這一行敘述，變成 ProxyPass /forum http://172.20.100.200/forum ProxyPassReverse /forum http://172.20.100.200/forum  compress \u0026lt;ifmodule mod_deflate.c\u0026gt; # 壓縮率，建議值:6 DeflateCompressionLevel 6 AddOutputFilterByType DEFLATE text/plain AddOutputFilterByType DEFLATE text/html AddOutputFilterByType DEFLATE text/xml AddOutputFilterByType DEFLATE text/css AddOutputFilterByType DEFLATE text/javascript AddOutputFilterByType DEFLATE application/xhtml+xml AddOutputFilterByType DEFLATE application/xml AddOutputFilterByType DEFLATE application/rss+xml AddOutputFilterByType DEFLATE application/atom_xml AddOutputFilterByType DEFLATE application/x-javascript AddOutputFilterByType DEFLATE application/x-httpd-php AddOutputFilterByType DEFLATE image/svg+xml \u0026lt;/ifmodule\u0026gt;  Multiple SSL Certificates NameVirtualHost *:443 \u0026lt;VirtualHost *:443\u0026gt; ServerName www.yoursite.com DocumentRoot /var/www/site SSLEngine on SSLCertificateFile /path/to/www_yoursite_com.crt SSLCertificateKeyFile /path/to/www_yoursite_com.key SSLCertificateChainFile /path/to/DigiCertCA.crt \u0026lt;/VirtualHost\u0026gt; \u0026lt;VirtualHost *:443\u0026gt; ServerName www.yoursite2.com DocumentRoot /var/www/site2 SSLEngine on SSLCertificateFile /path/to/www_yoursite2_com.crt SSLCertificateKeyFile /path/to/www_yoursite2_com.key SSLCertificateChainFile /path/to/DigiCertCA.crt \u0026lt;/VirtualHost\u0026gt;  expires \u0026lt;IfModule mod_expires.c\u0026gt; ExpiresActive on ExpiresDefault \u0026quot;access plus 1 week\u0026quot; ​ # CSS ExpiresByType text/css \u0026quot;access plus 1 week\u0026quot; ​ # Data interchange ExpiresByType application/json \u0026quot;access plus 0 seconds\u0026quot; ExpiresByType application/xml \u0026quot;access plus 0 seconds\u0026quot; ExpiresByType text/xml \u0026quot;access plus 0 seconds\u0026quot; ​ # Favicon (cannot be renamed!) and cursor images ExpiresByType image/x-icon \u0026quot;access plus 1 week\u0026quot; ​ # HTML components (HTCs) ExpiresByType text/x-component \u0026quot;access plus 1 week\u0026quot; ​ # HTML ExpiresByType text/html \u0026quot;access plus 0 seconds\u0026quot; ​ # JavaScript ExpiresByType application/javascript \u0026quot;access plus 1 week\u0026quot; ​ # Manifest files ExpiresByType application/x-web-app-manifest+json \u0026quot;access plus 0 seconds\u0026quot; ExpiresByType text/cache-manifest \u0026quot;access plus 0 seconds\u0026quot; ​ # Media ExpiresByType audio/ogg \u0026quot;access plus 1 week\u0026quot; ExpiresByType image/gif \u0026quot;access plus 1 week\u0026quot; ExpiresByType image/jpeg \u0026quot;access plus 1 week\u0026quot; ExpiresByType image/png \u0026quot;access plus 1 week\u0026quot; ExpiresByType video/mp4 \u0026quot;access plus 1 week\u0026quot; ExpiresByType video/ogg \u0026quot;access plus 1 week\u0026quot; ExpiresByType video/webm \u0026quot;access plus 1 week\u0026quot; # Web feeds ExpiresByType application/atom+xml \u0026quot;access plus 1 hour\u0026quot; ExpiresByType application/rss+xml \u0026quot;access plus 1 hour\u0026quot; \u0026lt;/IfModule\u0026gt;  HSTS( HTTP Strict Transport Security ) Header always set Strict-Transport-Security \u0026quot;max-age=31536000;includeSubdomains; preload\u0026quot;  "
},
{
	"uri": "https://blog.twasa.cf/devops/ansible/",
	"title": "Ansible",
	"tags": ["Automation", "Devops"],
	"description": "",
	"content": " Concept  功能：IT automation tool. It can configure systems, deploy software, and orchestrate more advanced IT tasks such as continuous deployments or zero downtime rolling updates. 管理方式：push-based Ansible manages machines in an agent-less manner. Ansible by default manages machines over the SSH protocol. Because OpenSSH is one of the most peer-reviewed open source components, security exposure is greatly reduced.  Requirement  Control Machine: SSH client and Linux system Managed Node: Python 2.5+ and SSH service, or windows supprt winrm  Glossary  Control Machine Managed Node inventory  定義Managed Node主機位址與群組 設定SSH 連線資訊、SSH金鑰、使用者名稱\u0026hellip;.等  Ad-Hoc command: 簡短一次性的指令 PlayBook: 使用YAML格式撰寫的腳本，可使用Jinja(template系統)  一個PlayBook可有多個Play跟Task Play: 要跑的大項目標，與Managed Node Task: 要做的工作細項 module: 已寫好的自動化模組 Roles: 是一種分類 \u0026amp; 重用的概念，透過將 vars, tasks, files, templates, handler … 等等根據不同的目的(例如：web server、db server)，規劃後至於獨立目錄中，後續便可以利用 include 的概念來使用。  Galayx: 是一個搜尋、分享與下載 roles的網站 facts: 實際上是ansible的setup module功能，用來取得Managed Node的系統資訊  SSH connection issue  關閉SSH key host 檢查：在ansible.cfg內 host_key_checking = False 關閉gathering facts: 所有playbook不管有沒有設定gathering facts tasks，都會執行，可以在playbook中加入 gather_facts: no SSH PIPElinING: 預設為關閉，所以關閉的原因是要相容不同的 sudo設定，若不使用sudo可以在ansible.cfg內開啟 pipelining=True ControlPersist: 即持久化socket一次驗證，多次通信，只需要修改SSH client也就是Ansible Control Machine本身的SSH 設定 ~/.ssh/config  Host * Compression yes TCPKeepAlive yes ServerAliveInterval 120 ServerAliveCountMax 5 ControlMaster auto ControlPath ~/.ssh/sockets/%r@%h-%p ControlPersist 1m  Speed Up Ansible  SSH multiplexing  [ssh_connection] ssh_args = -o ControlMaster=auto -o ControlPersist=60s   Pipelining  [ssh_connection] pipelining = true\n UseDNS  UseDNS is an SSH-server setting (/etc/ssh/sshd_config file) which forces a server to check a client\u0026rsquo;s PTR-record upon connection. It may cause connection delays especially with slow DNS servers on the server side. In modern Linux distribution, this setting is turned off by default, which is correct.\n PreferredAuthentications  It is an SSH-client setting which informs server about preferred authentication methods. By default Ansible uses:\n-o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey  So if GSSAPIAuthenticationis enabled on the server (at the time of writing this it is turned on in RHEL EC2 AMI) it will be tried as the first option, forcing the client and server to make PTR-record lookups. But in most cases, we want to use only public key auth. We can force Ansible to do so by changing ansible.cfg:\n[ssh_connection] ssh_args = -o ControlMaster=auto -o ControlPersist=60s -o PreferredAuthentications=publickey   Facts Gathering  gather_facts: no   Fork  The default value is 5, which is quite conservative. You can experiment with this setting depending on your local CPU and network bandwidth resources.\n[defaults] forks = 20   Poll Interval  When module is executed on remote host, Ansible starts to poll for its result. The lower is interval between poll attempts, the higher is CPU load on Ansible control host. But we want to have CPU available for greater forks number (see above). You can tweak poll interval in ansible.cfg: If you run \u0026ldquo;slow\u0026rdquo; jobs (like backups) on multiple hosts, you may want to increase the interval to 0.05 to use less CPU.\n[defaults] internal_poll_interval = 0.001  module module_name and arguments ping 無參數 comand -a 'ifconfig' user -a 'name= state={present(創建)|absent(刪除)} force=(是否強制操作刪除傢目錄) system= uid= shell= home=' group -a 'name= state={present|absent} gid= system=(系統組)' cron -a 'name= state= minute= hour= day= month= weekday= job=' file -a 'path= mode= owner= group= state={file|directory|link|hard|touch|absent} src=(link，鏈接至何處)' copy -a 'dest=(遠程主機上路徑) src=(本地主機路徑) content=(直接指明內容) owner= group= mode=' yum -a 'name= state={present(已安裝)|latest(最新版)|absent(未安裝)}' service -a 'name= state=started|restarted|stopped|reloaded' unarchive -a 'src= dest= remote_src={True|False}' lineinfile -a '' setup 無參數  inventory for all hosts ssh settings [all:vars] ansible_connection=ssh ansible_ssh_user='{{ user }}' ansible_ssh_pass='{{ password }}' ansible_become_pass='{{ password }}'  inventory for Differentiate Staging vs Production # file: production [atlanta-webservers] www-atl-1.example.com www-atl-2.example.com [boston-webservers] www-bos-1.example.com www-bos-2.example.com [atlanta-dbservers] db-atl-1.example.com db-atl-2.example.com [boston-dbservers] db-bos-1.example.com # webservers in all geos [webservers:children] atlanta-webservers boston-webservers # dbservers in all geos [dbservers:children] atlanta-dbservers boston-dbservers # everything in the atlanta geo [atlanta:children] atlanta-webservers atlanta-dbservers # everything in the boston geo [boston:children] boston-webservers boston-dbservers  syntax ansible syntax ansible \u0026lt;Patterns\u0026gt; -m \u0026lt;module_name\u0026gt; -a \u0026lt;arguments\u0026gt; \u0026lt;Options\u0026gt; Options: --list-hosts outputs a list of matching hosts --module-name module name to execute (default=command) --args module arguments --user connect as this user --ask-pass Prompt for the connection password --become Use privilege escalation --ask-become-pass Ask for privilege escalation password --inventory The PATH to the inventory, which defaults to /etc/ansible/hosts --limit further limit selected hosts to an additional pattern or comma separated host list. --check Check mode is just a simulation it will not make any changes on remote systems --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) --background= run asynchronously, failing after X seconds(default=N/A) --poll set the poll interval if using -B (default=15) --forks specify number of parallel processes to use(default=5) --extra-vars Extra variables to inject into a playbook, in key=value key=value format or as quoted YAML/JSON (hashes and arrays). To load variables from a file, specify the file preceded by @ (e.g. @vars.yml).  example ansible localhost -m ping #連本機自己,無須驗證 ansible localhost -m ping -i \u0026quot;localhost,\u0026quot; -u 帳號 -k 密碼 --key-file=私鑰檔案  ansible-playbook syntax ansible-playbook playbook.yml \u0026lt;Options\u0026gt; Options: --check Check mode is just a simulation it will not make any changes on remote systems --inventory The PATH to the inventory, which defaults to /etc/ansible/hosts --limit further limit selected hosts to an additional pattern --list-hosts outputs a list of matching hosts --syntax-check perform a syntax check on the playbook, but do not execute it --tags=TAGS only run plays and tasks tagged with these values --flush-cache clear the fact cache  ansible-vault syntax ansible-vault [create|decrypt|edit|encrypt|rekey|view] [--help] [options] vaultfile.yml Options: create foo.yml 建立加密 (Encrypted) 檔案。 edit foo.yml 編輯加密檔案內容。 rekey foo.yml 更換加密金鑰 (密碼)。 encrypt foo.yml 對已存在的明文檔案進行加密 decrypt foo.yml 解開 (Decrypt) 已加密檔案。 view foo.yml 檢視已加密的檔案內容。  ansible playboox examples service - hosts: all gather_facts: no tasks: - name: ensure enable_twrd is running service: name=enable_twrd state=started  shell - hosts: all gather_facts: no tasks: - name: enable twrd account shell: /etc/init.d/enable_twrd start - name: check twrd status shell: /etc/init.d/enable_twrd status register: ps - debug: var=ps.stdout_lines  copy, unzip, file - hosts: all gather_facts: no tasks: - name: copy news archive file to target news path copy: src: /root/news.zip dest: /mydlink/portal/web/_news/news.zip - name: unzip news archive file to target news path unarchive: src: /mydlink/portal/web/_news/news.zip dest: /mydlink/portal/web/_news/ remote_src: True - name: change owner and permission to news files file: path: /mydlink/portal/web/_news/ owner: webuser group: daemon mode: 0750 recurse: yes  Handlers, If nothing notifies a handler, it will not run - name: template configuration file template: src=template.j2 dest=/etc/foo.conf notify: - restart memcached - restart apache handlers: - name: restart memcached service: name=memcached state=restarted - name: restart apache service: name=apache state=restarted  retry task  Retry task 10 times with interval 1 second until return code of the command will not be 0. Ignore if even all tries will fail.  - hosts: all connection: local tasks: - shell: exit 1 register: task_result until: task_result.rc == 0 retries: 10 delay: 1 ignore_errors: yes  Delegation, Rolling Updates, and Local Actions  By default, Ansible will try to manage all of the machines referenced in a play in parallel. For a rolling updates use case, you can define how many hosts Ansible should manage at a single time by using the ‘’serial’’ keyword: examples:  - name: test play hosts: webservers service: name=httpd state=started serial: \u0026quot;30%\u0026quot; --- - name: test play hosts: webservers service: name=httpd state=started serial: 3 --- - name: test play hosts: webservers serial: - 1 - 5 - \u0026quot;20%\u0026quot;  ansible-playbook with sudo and vaults  hosts  [all:vars] ansible_connection=ssh ansible_ssh_user='{{ ansible_ssh_user }}' ansible_ssh_pass='{{ ansible_ssh_pass }}' ansible_become_pass='{{ ansible_become_pass }}'   play\n```yaml  hosts: all gather_facts: no tasks: name: restart sshd service shell: /etc/init.d/sshd restart name: check sshd status ```  vaults\n ansible-vault edit YOUR-VAULT-FILE\n  ansible_ssh_user: YOUR_USER_NAME ansible_ssh_pass: 'YOUR_PASSWORD' ansible_become_pass: 'YOUR_SUDO_PASSWORD'   run playbook with log output  echo \u0026quot;`ansible-playbook YOUR_PLAYBOOK.yml --inventory \u0026quot;localhost,\u0026quot; --user --ask-ssh-pass --become --ask-become-pass --ask-vault-pass -e@YOUR_VAULT_FILE -vvv`\u0026quot; | tee -a LOG-FILE-PATH  "
},
{
	"uri": "https://blog.twasa.cf/programing/python/python-web-automation-test/",
	"title": "Python Web Automation Test",
	"tags": ["Development", "Selenium"],
	"description": "",
	"content": " Requirements  Python Selenium webdriver HTMLTestRunner  Sample code, test login to bugtracker and create html report from selenium import webdriver import unittest import time import HTMLTestRunner class test_class(unittest.TestCase): def setUp(self): self.verificationErrors=[] self.test=webdriver.Ie() self.url=\u0026quot;http://your.bugtracker.com/bug/\u0026quot; def test_login(self): pa=self.test pa.get(self.url) user=pa.find_element_by_id('username') user.send_keys('使用者帳號') passwd=pa.find_element_by_id('password') passwd.send_keys('密碼') pa.execute_script('loginsubmit()') time.sleep(10) def tearDown(self): pass if __name__==\u0026quot;__main__\u0026quot;: testsuite=unittest.TestSuite() testsuite.addTest(test_class(\u0026quot;test_login\u0026quot;)) filename=\u0026quot;r:\\\\result.html\u0026quot; fp=file(filename,'wb') runner=HTMLTestRunner.HTMLTestRunner(stream=fp,title='Result',description='Test_Report') runner.run(testsuite)  "
},
{
	"uri": "https://blog.twasa.cf/programing/python/python-soap-client-suds/",
	"title": "A Python SOAP client sample code",
	"tags": ["Development"],
	"description": "",
	"content": " suds API references\n https://pypi.python.org/pypi/suds https://fedorahosted.org/suds/wiki/Documentation  example code\n  import logging logging.basicConfig(level=logging.INFO) logging.getLogger('suds.client').setLevel(logging.DEBUG) username = '' password = '' no = 20787 def b64enc(username, password): import base64 base64string = base64.encodestring('%s:%s' % (username, password)).replace('\\n', '') authenticationHeader = {\u0026quot;SOAPAction\u0026quot; : \u0026quot;ActionName\u0026quot;, \u0026quot;Authorization\u0026quot; : \u0026quot;Basic %s\u0026quot; % base64string } return authenticationHeader def xmlout(xmlstr): filename = '%s.xml' % no f = open(filename, \u0026quot;w\u0026quot;) f.write(xmlstr.encode('utf8')) def main(): from suds.client import Client wsdl_url = 'http://example.com:6666/portal-ws/ws/PersonalService?wsdl' authenticationHeader = b64enc(username, password) client = Client(url=wsdl_url, headers=authenticationHeader, retxml=True) #list_methods_of_service = [method for method in client.wsdl.services[0].ports[0].methods] #using service.findByNo methods response = client.service.findByNo(no) xml = response.decode('utf8') print xml xmlout(xml) if __name__ == '__main__': main()  "
},
{
	"uri": "https://blog.twasa.cf/programing/python/python-package/",
	"title": "Python Package",
	"tags": ["Development"],
	"description": "",
	"content": " description  套件 (package) 是 Python 用資料夾組織模組 (module) 檔案的方式  例如我們寫了三個模組 module01.py module02.py module03.py\n三個模組檔案放在 package01資料夾中，package01就是套件名稱 這時package01資料夾需要有額外一個 init.py init.py 可以不需要有任何內容 因為需要這個檔案，直譯器才能辨識package01為套件\n另外寫一個 main.py 來利用 package01 中 module01.py 所定義的class(類別)或function(函數) 注意main.py 必須放在與 package 相同路徑下檔案結構如下\nc:\\packagedemo\\package\\module01.py c:\\packagedemo\\package\\module03.py c:\\packagedemo\\package\\module04.py c:\\packagedemo\\main.py  在main.py引入套件的部份，XXX可以是module01裡面的class(類別)或function(函數) from package.module01 import XXX\n那麼 init.py 除了讓直譯器識別資料夾為套件之外有什麼作用呢？這裡，我們可以在 init.py 裡提供一個 all 變數，如下 all = [\u0026ldquo;module01\u0026rdquo;, \u0026ldquo;module02\u0026rdquo;, \u0026ldquo;module03\u0026rdquo;];\n當 all 被定義後，寫 from package import *\n才能保證 module01 、 module02 及 module03 三個模組都被引入。\n"
},
{
	"uri": "https://blog.twasa.cf/post/cisco-ios-netwrok-cmmmands/",
	"title": "Cisco Ios Netwrok Cmmmands",
	"tags": ["Cisco"],
	"description": "",
	"content": " 狀態查看  show startup-config show running-config show version show vlan id show ip route show ip nhrp show crypto engine connection active sh run | inc ip route 查看連線的TCP session  show ip sockets detail  查IP流量  conf t interface xxx ip accounting end sh ip accounting 備註, 查完記得no掉, 否則會增加cpu的負擔! conf t interface xxx no ip accounting   基本操作  從User Mode進入Privileged Mode enable 從Privileged Mode進入Global Configuration Mode configure terminal 儲存設定  write or copy running-config startup-config   設定  介面IP設定 interface fastEthernet 0/0 ip address IP位置 子網路遮罩 no shutdown\n 路由設定\n ip route IP位置 子網路遮罩 路由IP位置\n ip route IP位置 子網路遮罩 介面名稱\n PPPoE設定\n interface Dialer1\n mtu 1492\n ip address negotiated\n ip nat outside\n no ip virtual-reassembly\n encapsulation ppp\n ip tcp adjust-mss 1300\n dialer pool 1\n dialer-group 1\n ppp pap sent-username XXXX password 0 XXXX\n exit\n interface FastEthernet x/x\n description PPPOE\n no ip address\n duplex auto\n speed auto\n pppoe enable group global\n pppoe-client dial-pool-number 1\n  連線管理設定  line vty 0 4 tra in telnet ssh  AAA設定  aaa new-model aaa authentication login AAAXXX local group radius aaa authorization console aaa authorization exec AAAXXX local group radius aaa session-id common radius-server host A.B.C.D auth-port XXXX acct-port XXXX radius-server key 7 XXXXXX line vty 0 4 authorization exec AAAXXX login authentication AAAXXX exit line vty 5 15 authorization exec AAAXXX login authentication AAAXXX exit ip radius source-interface 介面名稱  權限設定  privilege exec level 7 telnet privilege exec level 7 ping ip privilege exec level 7 ping privilege exec level 7 show startup-config privilege exec level 7 show running-config privilege exec level 7 show  config file backup to FTP 設定  archive path ftp://10.192.1.139/SOME_PATH/$h write-memory time-period 1440 exit ip ftp source-interface 介面名稱 ip ftp username XXXX ip ftp password 7 XXXXXXXX exit  連線測試  ping 10.192.1.4 source 介面名稱 ping 10.192.1.139 source 介面名稱  NetFlow CE 設定  snmp-server community cnlinktw RO show ip flow export : Shows the current NetFlow configuration interface Loopback0 description Netflow ip address A.B.C.D 255.255.255.255 ip flow-export source Loopback0 ip flow-export version 5 ip flow-export destination collectorIP位址collectorPort  Router GRE Tunnel 設定  interface Tunnel xxx description \u0026ldquo;Primary Tunnel description\u0026rdquo; ip address IP位址 子網路遮罩 tunnel source IP位址 tunnel destination IP位址  NHRP CE 設定  interface Tunnel1009 description \u0026ldquo;NHRP description\u0026rdquo; ip address NHCsPrivateIP位址 255.255.255.252 no ip redirects ip nhrp authentication tp001542 ip nhrp map 10.195.252.26 NHSsPublicIP位址 ip nhrp map multicast NHSsPublicIP位址 ip nhrp network-id 1542 ip nhrp holdtime 60 ip nhrp nhs NHSsIP ip tcp adjust-mss 1460 tunnel source 介面名稱 tunnel mode gre multipoint tunnel key 1542  "
},
{
	"uri": "https://blog.twasa.cf/linux/sed/",
	"title": "sed",
	"tags": ["command"],
	"description": "",
	"content": " reference  https://www.gnu.org/software/sed/manual/sed.html http://www.gnu.org/software/sed/manual/sed.html#Examples  syntax sed OPTIONS... [SCRIPT] [INPUTFILE...] STDOUT | sed OPTIONS... [SCRIPT]  OPTIONS -i edited in-place  SCRIPT format '起始行數,結束行數 指令/要被取代的字串/新字串/旗標'  指令： s 搜尋並取代\n旗標 g 取代全部 c 取代前詢問是否confirm I 忽略 pattern 大小寫\nexample modify CentOS linux hostname sed -i 's/HOSTNAME\\=.*/HOSTNAME\\=NEWHOSTNAME/g' /etc/sysconfig/network  change default delimiter 改用 \u0026ldquo;:\u0026rdquo; sed 's:orig:NEW:'  改用 \u0026ldquo;#\u0026rdquo; 當delimiter sed -e 's#/#\\\\#g'  "
},
{
	"uri": "https://blog.twasa.cf/post/apache-tomcat/",
	"title": "Apache Tomcat",
	"tags": ["Apache", "Tomcat"],
	"description": "",
	"content": " 參考文件  https://tomcat.apache.org/tomcat-8.0-doc/index.html https://tomcat.apache.org/tomcat-8.0-doc/config/  install java, check it running and version yum install java-1.8.0-openjdk or curl -v -j -k -L -H \u0026quot;Cookie: oraclelicense=accept-securebackup-cookie\u0026quot; http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.rpm \u0026gt; jdk-8u112-linux-x64.rpm \u0026amp;\u0026amp; rpm -Uvh jdk-8u112-linux-x64.rpm java -version  create a nologin user for running tomcat useradd -s /sbin/nologin tomcat  download from apache tomcat cd /home/tomcat wget http://apache.stu.edu.tw/tomcat/tomcat-8/v8.0.28/bin/apache-tomcat-8.0.28.tar.gz tar zxvf apache-tomcat-8.0.28.tar.gz  setenv.sh  設定運行參數如記憶體、垃圾回收機制等參數\nJAVA_OPTS=\u0026quot;-Dfile.encoding=UTF-8 -Dnet.sf.ehcache.skipUpdateCheck=true -server -Xms1024m -Xmx1024m -Xmn128m -Xss2m -XX:+UseParallelGC\u0026quot;  設定Tomcat JMX remote 監控\nCATALINA_OPTS=\u0026quot;-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=8888 -Dcom.sun.management.jmxremote.rmi.port=8889\u0026quot;   catalina.properties  設定增加讓webapps可讀取property檔路徑  common.loader=\u0026quot;${catalina.base}/lib\u0026quot;,\u0026quot;${catalina.base}/lib/*.jar\u0026quot;,\u0026quot;${catalina.home}/lib\u0026quot;,\u0026quot;${catalina.home}/lib/*.jar\u0026quot;,\u0026quot;${catalina.home}/路徑\u0026quot;  server.xml  設定webapp可存取系統目錄 設定SSL   \u0026lt;Connector port=\u0026quot;4433\u0026quot; maxThreads=\u0026quot;200\u0026quot; scheme=\u0026quot;https\u0026quot; secure=\u0026quot;true\u0026quot; SSLEnabled=\u0026quot;true\u0026quot; keystoreFile=\u0026quot;/path/to/your/keyfile\u0026quot; keystorePass=\u0026quot;密碼\u0026quot; clientAuth=\u0026quot;false\u0026quot; sslProtocol=\u0026quot;TLS\u0026quot;/\u0026gt;  tomcat-users.xml  設定使用者或manager-script(deploy server using like jenkins)可存取Manager頁面   \u0026lt;role rolename=\u0026quot;manager-gui\u0026quot; /\u0026gt; \u0026lt;role rolename=\u0026quot;manager-script\u0026quot; /\u0026gt; \u0026lt;role rolename=\u0026quot;manager-jmx\u0026quot; /\u0026gt; \u0026lt;user username=\u0026quot;帳號\u0026quot; password=\u0026quot;密碼\u0026quot; roles=\u0026quot;manager-gui\u0026quot; /\u0026gt; \u0026lt;user username=\u0026quot;帳號\u0026quot; password=\u0026quot;密碼\u0026quot; roles=\u0026quot;standard,manager-script,manager-jmx\u0026quot; /\u0026gt; roles 說明 manager-gui - allows access to the HTML GUI and the status pages manager-script - allows access to the text interface and the status manager-jmx - allows access to the JMX proxy and the status manager-status - allows access to the status pages only  context.xml  設定資料庫連線位址、Port、帳號、密碼\u0026hellip;..   \u0026lt;Resource name=\u0026quot;jdbc/shacomBid\u0026quot; auth=\u0026quot;Container\u0026quot; type=\u0026quot;javax.sql.DataSource\u0026quot; maxActive=\u0026quot;10\u0026quot; maxIdle=\u0026quot;5\u0026quot; maxWait=\u0026quot;10000\u0026quot; username=\u0026quot;帳號\u0026quot; password=\u0026quot;密碼\u0026quot; driverClassName=\u0026quot;com.mysql.jdbc.Driver\u0026quot; url=\u0026quot;jdbc:mysql://DBFQDN:3306/bid?characterEncoding=UTF-8\u0026amp;allowMultiQueries=true\u0026amp;useUnicode=true\u0026quot;/\u0026gt;  設定Manager頁面IPv4網段存取限制 cp -p /home/apache-tomcat-X.X.XX/webapps/host-manager/manager.xml /home/apache-tomcat-X.X.XX/conf/Catalina/localhost/ vi /home/apache-tomcat-X.X.XX/conf/Catalina/localhost/manager.xml \u0026lt;Context docBase=\u0026quot;${catalina.home}/webapps/manager\u0026quot; privileged=\u0026quot;true\u0026quot; antiResourceLocking=\u0026quot;false\u0026quot; antiJARLocking=\u0026quot;false\u0026quot;\u0026gt; \u0026lt;Valve className=\u0026quot;org.apache.catalina.valves.RemoteAddrValve\u0026quot; allow=\u0026quot;A\\.B\\.C\\.\\d+|127\\.\\d+\\.\\d+\\.\\d+\u0026quot; denyStatus=\u0026quot;404\u0026quot; /\u0026gt; cp -p /home/apache-tomcat-X.X.XX/webapps/host-manager/manager.xml /home//apache-tomcat-X.X.XX/conf/Catalina/localhost/ROOT.xml vi /home/apache-tomcat-X.X.XX/conf/Catalina/localhost/ROOT.xml \u0026lt;Context docBase=\u0026quot;${catalina.home}/webapps/ROOT\u0026quot; privileged=\u0026quot;true\u0026quot; antiResourceLocking=\u0026quot;false\u0026quot; antiJARLocking=\u0026quot;false\u0026quot;\u0026gt; \u0026lt;Valve className=\u0026quot;org.apache.catalina.valves.RemoteAddrValve\u0026quot; allow=\u0026quot;A\\.B\\.C\\.\\d+|127\\.\\d+\\.\\d+\\.\\d+\u0026quot; denyStatus=\u0026quot;404\u0026quot; /\u0026gt;  設定manager deploy war檔案大小  vi /home/apache-tomcat-X.X.XX/webapps/manager/WEB-INF/web.xml \u0026lt;multipart-config\u0026gt; \u0026lt;max-file-size\u0026gt;152428800\u0026lt;/max-file-size\u0026gt; \u0026lt;max-request-size\u0026gt;152428800\u0026lt;/max-request-size\u0026gt; \u0026lt;file-size-threshold\u0026gt;0\u0026lt;/file-size-threshold\u0026gt; \u0026lt;/multipart-config\u0026gt;  "
},
{
	"uri": "https://blog.twasa.cf/db/my-db/",
	"title": "MySQL and MariaDB memo",
	"tags": ["MySQL", "MariaDB"],
	"description": "",
	"content": " 第一次安裝完成，設定root的密碼 mysqladmin -u root -p \u0026lsquo;密碼\u0026rsquo; or mysql_secure_installation\n重設root密碼 /etc/init.d/mysql stop mysqld_safe --skip-grant-tables \u0026amp; mysql -u root mysql\u0026gt; use mysql; mysql\u0026gt; UPDATE user SET Password=PASSWORD(\u0026quot;密碼\u0026quot;) WHERE User='root'; mysql\u0026gt; flush privileges; mysql\u0026gt; quit /etc/init.d/mysql stop /etc/init.d/mysql start  連線管理資料庫 mysqladmin -u root -p Enter password: 此時再輸入密碼(建議採用) 修改使用者密碼 方法一 使用有權限或要修改的使用者本身登入mysql mysql\u0026gt; SET PASSWORD FOR '目標使用者'@'主機' = PASSWORD('密碼'); mysql\u0026gt; flush privileges 方法二 使用有權限的使用者登入mysql 修改使用者密碼，只改 root 的密碼，如果沒有用 where ，則表示改全部 user 的密碼 mysql\u0026gt; use mysql; mysql\u0026gt; UPDATE user SET password=password('密碼') where user='root'; mysql\u0026gt; FLUSH PRIVILEGES; 上面是不分主機位址的修改，若要像方法一區分主機的話再加上Host條件，例如 mysql\u0026gt; UPDATE user SET Password=PASSWORD(\u0026quot;密碼\u0026quot;) WHERE User='root' AND Host = 'localhost'; mysql\u0026gt; FLUSH PRIVILEGES; 方法三 同樣利用mysqladmin指令可以修改root或其他使用者密碼，但該使用者必須有SUPER權限 mysqladmin -u 使用者 -p'舊密碼' password '新密碼' 忘記密碼重設 /etc/init.d/mysql stop mysqld_safe --skip-grant-tables \u0026amp; 用上面方式啟動mysql後可以不用輸入密碼直接連入 mysql -u root 接者使用修改使用者密碼的方法二修改root密碼，最後重新啟動mysql  新增root可遠端存取 %表示任何IP或只接輸入IP mysql\u0026gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '密碼' WITH GRANT OPTION; mysql\u0026gt; FLUSH PRIVILEGES; 資料庫(DateBase)十五種權限 ALL PRIVILEGES、ALTER、CREATE、DELETE、DROP、FILE、INDEX、INSERT、PROCESS、REFERENCES、RELOAD、SELECT、SHUTDOWN、UPDATE、USAGE 資料表(Table)八種權限 SELECT、INSERT、UPDATE、DELETE、CREATE、DROP、INDEX、ALTER 資料欄(column)三種權限 SELECT INSERT UPDATE  查詢現有User select user from mysql.user;  查詢MySQL 對 此帳號 開放(GRANT)哪些權限 查詢 某 User 的權限 SELECT User,Host FROM mysql.user; # 秀出系統現在有哪些 user SHOW GRANTS FOR username@localhost; # 會秀出開此 username 時下的 Grant 語法, 也可用此來做帳號備份. 結果: GRANT SELECT, INSERT, UPDATE, DELETE ON *.* TO 'username'@'localhost' IDENTIFIED BY PASSWORD 結果: GRANT USAGE ON *.* TO '帳號'@'192.168.88.%' IDENTIFIED BY PASSWORD ' GRANT SELECT, EXECUTE ON '資料庫'.* TO '帳號'@'192.168.88.%' 下述這些結果都一樣, 都是列出 目前此User 的權限. SHOW GRANTS; SHOW GRANTS FOR CURRENT_USER; SHOW GRANTS FOR CURRENT_USER();  修改 mysql\u0026gt; update db set Host='202.54.10.20' where Db='webdb'; mysql\u0026gt; update user set Host='202.54.10.20' where user='webadmin';  刪除 mysql\u0026gt; DELETE FROM mysql.user WHERE User = 'root' AND Host = '%'; mysql\u0026gt; FLUSH PRIVILEGES;  刪除空帳號 mysql\u0026gt; DELETE FROM user WHERE User = ''; mysql\u0026gt; FLUSH PRIVILEGES;  建立新帳號 GRANT SELECT,INSERT,UPDATE ON datab_name.* TO user@host IDENTIFIED BY 'passwd'; GRANT ALL ON *.* TO root@'10.99.1.%' IDENTIFIED BY '密碼'; mysql\u0026gt; GRANT 權限 ON 資料庫或資料表 TO 使用者 IDENTIFIED BY '密碼'; 資料庫或資料表 *.*所有資料庫裡的所有資料表 *預設資料庫裡的所有資料表 資料庫.*某一資料庫裡的所有資料表 資料庫.資料表某一資料庫裡的特定資料表 資料表預設資料庫裡的某一資料表  範例 把 db35 這個資料庫(含其下的所有資料表)，授權給 s35，從 localhost 上來 mysql\u0026gt; GRANT all ON bugdb.* TO bug@'localhost' IDENTIFIED BY '密碼'; mysql -h host -u user -p  安全性設定(*.*為資料庫.資料表, @前面的*表示帳號, %可改成IP比如140.92.25.1) GRANT ALL PRIVILEGES ON *.* TO '*'@'%' IDENTIFIED BY '密碼' WITH GRANT OPTION; mysql\u0026gt; FLUSH PRIVILEGES; （最後一定要強迫更新權限） Permissible Privileges for GRANT and REVOKE Privilege Meaning ALL [PRIVILEGES] Grant all privileges at specified access level except GRANT OPTION ALTER Enable use of ALTER TABLE ALTER ROUTINE Enable stored routines to be altered or dropped CREATE Enable database and table creation CREATE ROUTINE Enable stored routine creation CREATE TEMPORARY TABLES Enable use of CREATE TEMPORARY TABLE CREATE USER Enable use of CREATE USER, DROP USER, RENAME USER, and REVOKE ALL PRIVILEGES CREATE VIEW Enable views to be created or altered DELETE Enable use of DELETE DROP Enable databases, tables, and views to be dropped EVENT Enable use of events for the Event Scheduler EXECUTE Enable the user to execute stored routines FILE Enable the user to cause the server to read or write files GRANT OPTION Enable privileges to be granted to or removed from other accounts INDEX Enable indexes to be created or dropped INSERT Enable use of INSERT LOCK TABLES Enable use of LOCK TABLES on tables for which you have the SELECT privilege PROCESS Enable the user to see all processes with SHOW PROCESSLIST REFERENCES Not implemented RELOAD Enable use of FLUSH operations REPLICATION CLIENT Enable the user to ask where master or slave servers are REPLICATION SLAVE Enable replication slaves to read binary log events from the master SELECT Enable use of SELECT SHOW DATABASES Enable SHOW DATABASES to show all databases SHOW VIEW Enable use of SHOW CREATE VIEW SHUTDOWN Enable use of mysqladmin shutdown SUPER Enable use of other administrative operations such as CHANGE MASTER TO, KILL, PURGE BINARY LOGS, SET GLOBAL, and mysqladmin debug command TRIGGER Enable trigger operations UPDATE Enable use of UPDATE USAGE Synonym for “no privileges”  顯示目前有幾個資料庫 mysql\u0026gt; SHOW DATABASES; +--------------------+ | Database | +--------------------+ | information_schema | | bugdb | | mysql | | test | +--------------------+ 3 rows in set (0.00 sec)  列出該資料庫所有資料表名稱 mysql\u0026gt; SHOW TABLES FROM 資料庫名 [LIKE ...];  列出該資料表所有欄位名稱 mysql\u0026gt; SHOW COLUMNS FROM table_name [LIKE ...]; mysql\u0026gt; SHOW COLUMNS FROM table_name FROM db_name [LIKE ...]; mysql\u0026gt; SHOW FIELDS FROM table_name [LIKE ...]; mysql\u0026gt; DESCRIBE table_name ; mysql\u0026gt; EXPLAIN table_name ;  查詢資料庫大小 SELECT table_schema \u0026quot;DB Name\u0026quot;, Round(Sum(data_length + index_length) / 1024 / 1024, 1) \u0026quot;DB Size in MB\u0026quot; FROM information_schema.tables GROUP BY table_schema; +--------------------+---------------+ | DB Name | DB Size in MB | +--------------------+---------------+ | bugdb | 2.5 | | information_schema | 0.0 | | mysql | 0.6 | +--------------------+---------------+  查詢MySQL VARIABLES mysql\u0026gt; SHOW VARIABLES LIKE '%log_bin%'; +---------------------------------+-------------------------------------------------+ | Variable_name | Value | +---------------------------------+-------------------------------------------------+ | log_bin | ON | | log_bin_basename | /rdsdbdata/log/binlog/mysql-bin-changelog | | log_bin_index | /rdsdbdata/log/binlog/mysql-bin-changelog.index | | log_bin_trust_function_creators | ON | | log_bin_use_v1_row_events | OFF | | sql_log_bin | ON | +---------------------------------+-------------------------------------------------+  建立資料庫; CREATE DATABASE 資料庫名;  使用資料庫 USE 資料庫名;  刪除資料庫 DROP DATABASE 資料庫名; DROP DATABASE [IF EXISTS] 資料庫名;  建立資料表 CREATE TABLE 資料表名 (欄位1 資料型態, 欄位2 資料型態, \u0026hellip;\u0026hellip;); CREATE TABLE 資料表名 (autono INT NOT NULL AUTO_INCREMENT PRIMARY KEY, RACKID varchar(10), RACKLEVEL varchar(10), KVMID varchar(10), CUSERY varchar(20), SERVERENAME varchar(50), SERVERCNAME varchar(50), SERVERTYPE varchar(50), OSTYPE varchar(50), IPADDRESS varchar(20), SERVICEINFO varchar(50), CPUTYPE varchar(50), RAM varchar(20), STORAGE varchar(20), CRID varchar(50), FNTYPE varchar(20)); 資料結構(type): 資料型態 說明 TINYINT 有符號的範圍是-128到127， 無符號的範圍是0到255。 SMALLINT 有符號的範圍是-32768到32767， 無符號的範圍是0到65535。 MEDIUMINT 有符號的範圍是-8388608到8388607， 無符號的範圍是0到16777215。 INT 有符號的範圍是-2147483648到2147483647， 無符號的範圍是0到4294967295。 INTEGER INT的同義詞。 BIGINT 有符號的範圍是-9223372036854775808到 9223372036854775807，無符號的範圍是0到18446744073709551615。 FLOAT 單精密浮點數字。不能無符號。允許的值是-3.402823466E+38到- 1.175494351E-38，0 和1.175494351E-38到3.402823466E+38。 DOUBLE 雙精密)浮點數字。不能無符號。允許的值是- 1.7976931348623157E+308到-2.2250738585072014E-308、 0和2.2250738585072014E-308到1.7976931348623157E+308。 DOUBLE PRECISION DOUBLE的同義詞。 REAL DOUBLE的同義詞。 DECIMAL DECIMAL值的最大範圍與DOUBLE相 同。 NUMERIC DECIMAL的同義詞。 DATE 日期。支援的範圍是'1000-01-01'到'9999-12-31'。 DATETIME 日期和時間組合。支援的範圍是'1000-01-01 00:00:00'到'9999-12-31 23:59:59' TIMESTAMP 時間戳記。範圍是'1970-01-01 00:00:00'到2037年的某時。 TIME 一個時間。範圍是'-838:59:59'到'838:59:59'。 YEAR 2或4位數字格式的年(內定是4位)。允許的值是1901到2155。 CHAR 固定長度，1 ～ 255個字元。 VARCHAR 可變長度，1 ～ 255個字元。 TINYBLOB TINYTEXT 最大長度為255(2^8-1)個字符。 MEDIUMBLOB MEDIUMTEXT 最大長度為16777215(2^24-1)個字符。 LONGBLOB LONGTEXT 最大長度為4294967295(2^32-1)個字符。 ENUM 一個ENUM最多能有65535不同的值。 SET 一個SET最多能有64個成員。  顯示表格 SHOW TABLES;  刪除資料表 DROP TABLE 資料表名; DROP TABLE [IF EXISTS] tbl_name [, tbl_name,...]  顯示表格結構 DESCRIBE infolist;  新增資料 INSERT INTO infolist (RACKID,RACKLEVEL,KVMID,CUSERY,SERVERENAME,SERVERCNAME,SERVERTYPE,OSTYPE,IPADDRESS,SERVICEINFO,CPUTYPE,RAM,STORAGE,CRID,FNTYPE) VALUES ('1','1','1','test','demo center','DynaManager-90','ASUS RS500-E6','CAKE v3.0.16 Alpha Final','140.92.25.6','Virtualization(DAS)','Intel(R) Xeon(R) CPU E5620 @ 2.40GH *2','24G','DAS 1TB','S/N:134IH11','DeSSerT');  Using System Variables SHOW VARIABLES; SHOW VARIABLES LIKE 'wait_timeout'; SHOW VARIABLES LIKE 'interactive_timeout'; SHOW VARIABLES LIKE 'connect_timeout';  備份某個資料庫 mysqldump -u root -p -h 主機 資料庫名 \u0026gt; 資料庫備份檔名  備份all資料庫 mysqldump -u root -p -h 主機 --all-databases  同時備份多個MySQL資料庫 mysqldump -u root -p -h 主機 –databases 資料庫名1 資料庫名2 資料庫名3 \u0026gt; 資料庫備份檔名  備份MySQL資料庫為帶刪除表的格式,能夠讓該備份覆蓋已有資料庫而不需要手動刪除原有資料庫. mysqldump -h 主機 -u root -p -–add-drop-table 資料庫名 \u0026gt; 資料庫備份檔名  僅備份資料庫結構 mysqldump -h 主機 -u root -p --no-data -–databases databasename1 databasename2 databasename3 \u0026gt; structurebackupfile.sql  備份MySQL資料庫某個(些)表 mysqldump -h 主機 -u root -p 資料庫名 specific_table1 specific_table2 \u0026gt; backupfile.sql  刪除資料庫的所有TABLES(DROP remove tables) mysql -h 主機 -u root -p -Nse 'show tables' 資料庫名 | while read table; do mysql -u root -p -e \u0026quot;drop table $table\u0026quot; 資料庫名; done  清空資料庫的所有TABLES(Truncate empty tables) mysql -h 主機 -u root -p -Nse 'SHOW TABLES' 資料庫名 | while read table; do mysql -u root -p -e \u0026quot;truncate table $table\u0026quot; 資料庫名; done  復原一個資料庫 (需先建好資料庫) mysql -u root -p -h 主機 資料庫名 \u0026lt; 資料庫備份檔  還原壓縮的MySQL資料庫 gunzip \u0026lt; 資料庫備份檔名.sql.gz | mysql -h 主機 -u root -p 資料庫名  將資料庫轉移到新伺服器 mysqldump -u -p databasename | mysql –host=*.*.*.* -C databasename  If you want to see only a specific variable, you can use this command. Obviously you’d want to replace the max_connect_errors in that command with the variable that you’re looking for. SHOW VARIABLES LIKE '%max_connect_errors%';  If you want to change the current state of a variable, you can do so easily with a command similar to this one: SET GLOBAL max_connect_errors=10000; Mysql\u0026gt; FLUSH HOSTS; mysqlbinlog –start-datetime=\u0026quot;2016-03-01 00:00:00\u0026quot; –stop-datetime=\u0026quot;2016-03-16 00:00:00\u0026quot; -d bid mysql-bin.0001* \u0026gt; /root/replay.sql  tips You need to use the -p flag to send a password. And it's tricky because you must have no space between -p and the password. $ mysql -h \u0026quot;server-name\u0026quot; -u \u0026quot;root\u0026quot; \u0026quot;-pXXXXXXXX\u0026quot; \u0026quot;database-name\u0026quot; \u0026lt; \u0026quot;filename.sql\u0026quot; If you use a space after -p it makes the mysql client prompt you interactively for the password, and then it interprets the next command argument as a database-name: $ mysql -h \u0026quot;server-name\u0026quot; -u \u0026quot;root\u0026quot; -p \u0026quot;XXXXXXXX\u0026quot; \u0026quot;database-name\u0026quot; \u0026lt; \u0026quot;filename.sql\u0026quot; Enter password: ERROR 1049 (42000): Unknown database 'XXXXXXXX' To avoid password prompt just create ~/.my.cnf file as follows: [client] #for local server use localhost #host=localhost host=10.0.1.100 user=vivek password=myPassword [mysql] pager=/usr/bin/less Then: $ mysql -h \u0026quot;server-name\u0026quot; \u0026quot;database-name\u0026quot; \u0026lt; \u0026quot;filename.sql\u0026quot;  slow query 查看Server設定 ：show variables ; 其中兩個參數，一個是slow_launch_time，一個是long_query_time。 slow_launch_time跟slow query log沒有任何關係, 它代表的是thread create的一個門檻值，long_query_time才是正確的。 查看Server運作的各種設定 ： show global status; 可以用 show variables like '%slow%';過濾要找的值。 測試slow query，可以在用select搭sleep指令： select sleep(2); 然後再去看slow query log  "
},
{
	"uri": "https://blog.twasa.cf/linux/regexp/",
	"title": "Regular Expression",
	"tags": [],
	"description": "",
	"content": " Regula Express  reference :     說明及範例 比對不成立之字串     /a/ 含字母 “a” 的字串，例如 “ab”, “bac”, “cba” “xyz”   /a./ 含字母 “a” 以及其後任一個字元的字串，例如 “ab”, “bac”（若要比對.，請使用 .） “a”, “ba”   /^xy/ 以 “xy” 開始的字串，例如 “xyz”, “xyab”（若要比對 ^，請使用 \\^） “axy”, “bxy”   /xy$/ 以 “xy” 結尾的字串，例如 “axy”, “abxy”以 “xy” 結尾的字串，例如 “axy”, “abxy” （若要比對 $，請使用 \\$） “xya”, “xyb”   [13579] 包含 “1” 或 “3” 或 “5” 或 “7” 或 “9” 的字串，例如：”a3b”, “1xy” “y2k”   [0-9] 含數字之字串 不含數字之字串   [a-z0-9] 含數字或小寫字母之字串 不含數字及小寫字母之字串   [a-zA-Z0-9] 含數字或字母之字串 不含數字及字母之字串   b[aeiou]t “bat”, “bet”, “bit”, “bot”, “but” “bxt”, “bzt”   [^0-9] 不含數字之字串（若要比對 ^，請使用 \\^） 含數字之字串   [^aeiouAEIOU] 不含母音之字串（若要比對 ^，請使用 \\^） 含母音之字串   [^\\^] 不含 “^” 之字串，例如 “xyz”, “abc” “xy^”, “a^bc”       正規表示式的特定字元 說明 等效的正規表示式     \\d 數字 [0-9]   \\D 非數字 [^0-9]   \\w 數字、字母、底線 [a-zA-Z0-9_]   \\W 非 \\w [^a-zA-Z0-9_]   \\s 空白字元 [ \\r\\t\\n\\f]   \\S 非空白字元 [^ \\r\\t\\n\\f]       正規表示式 說明     /a?/ 零或一個 a（若要比對? 字元，請使用 \\?）   /a+/ 一或多個 a（若要比對+ 字元，請使用 +）   /a*/ 零或多個 a（若要比對* 字元，請使用 *）   /a{4}/ 四個 a   /a{5,10}/ 五至十個 a   /a{5,}/ 至少五個 a   /a{,3}/ 至多三個 a   /a.{5}b/ a 和 b中間夾五個（非換行）字元       字元 說明 簡單範例      避開特殊字元 /A*/ 可用於比對 “A*”，其中 * 是一個特殊字元，為避開其特殊意義，所以必須加上 “\\”   ^ 比對輸入列的啟始位置 /^A/ 可比對 “Abcd” 中的 “A”，但不可比對 “aAb”   $ 比對輸入列的結束位置 /A$/ 可比對 “bcdA” 中的 “A”，但不可比對 “aAb”   * 比對前一個字元零次或更多次 /bo*/ 可比對 “Good boook” 中的 “booo”，亦可比對 “Good bk” 中的 “b”   + 比對前一個字元一次或更多次，等效於 {1,} /a+/ 可比對 “caaandy” 中的 “aaa”，但不可比對 “cndy”   ? 比對前一個字元零次或一次 /e?l/ 可比對 “angel” 中的 “el”，也可以比對 “angle” 中的 “l”   . 比對任何一個字元（但換行符號不算） /.n/ 可比對 “nay, an apple is on the tree” 中的 “an” 和 “on”，但不可比對 “nay”   (x) 比對 x 並將符合的部分存入一個變數 /(a) and (b)/ 可比對 “aaa and bb” 中的 “aaa” 和 “bb”，並將兩個比對得到的字串設定至變數 RegExp.$1 和 RegExp.$2。   xy 比對 x 或 y /ab/g 可比對 “aaa and bb” 中的 “aaa” 和 “bb”   {n} 比對前一個字元 n 次，n 為一個正整數 /a{3}/ 可比對 “lllaaalaa” 其中的 “aaa”，但不可比對 “aa”   {n,} 比對前一個字元至少 n 次，n 為一個正整數 /a{3,}/ 可比對 “aa aaa aaaa” 其中的 “aaa” 及 “aaaa”，但不可比對 “aa”   {n,m} 比對前一個字元至少 n 次，至多 m 次，m、n 均為正整數 /a{3,4}/ 可比對 “aa aaa aaaa aaaaa” 其中的 “aaa” 及 “aaaa”，但不可比對 “aa” 及 “aaaaa”   [xyz] 比對中括弧內的任一個字元 /[ecm]/ 可比對 “welcome” 中的 “e” 或 “c” 或 “m”   [^xyz] 比對不在中括弧內出現的任一個字元 /[^ecm]/ 可比對 “welcome” 中的 “w”、”l”、”o”，可見出其與 [xyz] 功能相反。（同時請注意 /^/ 與 [^] 之間功能的不同。）   [\\b] 比對退位字元（Backspace character） 可以比對一個 backspace ，也請注意 [\\b] 與 \\b 之間的差別   \\b 比對英文字的邊界，例如空格 例如 /\\bn\\w/ 可以比對 “noonday” 中的 ‘no’ ; /\\wy\\b/ 可比對 “possibly yesterday.” 中的 ‘ly’   \\B 比對非「英文字的邊界」 例如, /\\w\\Bn/ 可以比對 “noonday” 中的 ‘on’ , 另外 /y\\B\\w/ 可以比對 “possibly yesterday.” 中的 ‘ye’   \\cX 比對控制字元（Control character），其中 X 是一個控制字元 /\\cM/ 可以比對 一個字串中的 control-M   \\d 比對任一個數字，等效於 [0-9] /[\\d]/ 可比對 由 “0” 至 “9” 的任一數字 但其餘如字母等就不可比對   \\D 比對任一個非數字，等效於 [^0-9] /[\\D]/ 可比對 “w” “a”… 但不可比對如 “7” “1” 等數字   \\f 比對 form-feed 若是在文字中有發生 “換頁” 的行為 則可以比對成功   \\n 比對換行符號 若是在文字中有發生 “換行” 的行為 則可以比對成功   \\r 比對 carriage return    \\s 比對任一個空白字元（White space character），等效於 [ \\f\\n\\r\\t\\v] /\\s\\w*/ 可比對 “A b” 中的 “b”   \\S 比對任一個非空白字元，等效於 [^ \\f\\n\\r\\t\\v] /\\S/\\w* 可比對 “A b” 中的 “A”   \\t 比對定位字元（Tab）    \\v 比對垂直定位字元（Vertical tab）    \\w 比對數字字母字元（Alphanumerical characters）或底線字母（””），等效於 [A-Za-z0-9] /\\w/ 可比對 “.A _!9” 中的 “A”、”_”、”9″。   \\W 比對非「數字字母字元或底線字母」，等效於 [^A-Za-z0-9_] /\\W/ 可比對 “.A _!9” 中的 “.”、” “、”!”，可見其功能與 /\\w/ 恰好相反。   \\ooctal 比對八進位，其中octal是八進位數目 /\\oocetal123/ 可比對 與 八進位的ASCII中 “123” 所相對應的字元值。   \\xhex 比對十六進位，其中hex是十六進位數目 /\\xhex38/ 可比對 與 16進位的ASCII中 “38” 所相對應的字元。    "
},
{
	"uri": "https://blog.twasa.cf/linux/find/",
	"title": "find",
	"tags": ["command"],
	"description": "",
	"content": " find description  search for files in a directory hierarchy  syntax find [options] [path...] [expression] --newerXY Compares the timestamp of the current file with reference. The reference argument is normally the name of a file (and one of its timestamps is used for the comparison) but it may also be a string describing an absolute time. X and Y are placeholders for other letters, and these letters select which time belonging to how reference is used for the comparison find file modified newer than some daytimes find /var/log -newermt '2016-06-01T00:00:00' -name pattern Base of file name (the path with the leading directories removed) matches shell pattern pattern.The metacharacters ('*', '?', and '[]') match a '.' at the start of the base name (this is a change in findutils-4.2.2; see section STANDARDS CONFORMANCE below)  "
},
{
	"uri": "https://blog.twasa.cf/linux/xargs/",
	"title": "xargs",
	"tags": ["command"],
	"description": "",
	"content": " xargs description  build and execute command lines from standard input  syntax and examples  Find all .log files in or below the current directory and process them  find . -name \u0026quot;*.log\u0026quot; -type f -print | xargs tar -cvf logs.tar find . -name \u0026quot;*.log\u0026quot; -type f -print | xargs -i -p cp -a {} /some/place find . -name \u0026quot;*.log\u0026quot; -type f -print | xargs -I [] cp -a [] /some/place -p Prompt the user about whether to run each command line and read a line from the terminal. Only run the command line if the response starts with 'y' or 'Y'. -I replace-str Replace occurrences of replace-str in the initial-arguments with names read from standard input. Also, unquoted blanks do not terminate input items; instead the separator is the newline character. Implies -x and -L 1. -i[replace-str] This option is a synonym for -Ireplace-str if replace-str is specified, and for -I{} otherwise. This option is deprecated; use -I instead.  "
},
{
	"uri": "https://blog.twasa.cf/linux/grep/",
	"title": "Grep",
	"tags": ["command"],
	"description": "",
	"content": " description  prints lines that contain a match for a pattern reference  https://www.gnu.org/software/grep/manual/grep.html   syntax grep [OPTIONS] PATTERN [FILE...] grep multiple patterns OR grep 'pattern1\\|pattern2\\|pattern3' AND grep -E 'pattern1.*pattern2.*pattern3' filename -v, --invert-match Invert the sense of matching, to select non-matching lines. (-v is specified by POSIX .) grep -v 'unwanted_word' -i, --ignore-case Ignore case distinctions in both the PATTERN and the input files. (-i is specified by POSIX .)  "
},
{
	"uri": "https://blog.twasa.cf/post/lnmp/",
	"title": "CentOS 7 Linux+Nginx+MariaDB+PHP",
	"tags": ["Nginx"],
	"description": "",
	"content": " Build a Linux, Nginx, MariaDB, PHP environment in CentOS 7 install rpm -Uvh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm yum install nginx php php-fpm mariadb mariadb-server  firewalld for http, https access only firewall-cmd --zone=public --list-all firewall-cmd --permanent --zone=public --add-service=http firewall-cmd --permanent --zone=public --add-service=https firewall-cmd --reload firewall-cmd --zone=public --list-all  Nginx config basic config, edit /etc/nginx/nginx.conf user nginx; worker_processes 2; #by your cpu error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \u0026quot;$request\u0026quot; ' '$status $body_bytes_sent \u0026quot;$http_referer\u0026quot; ' '\u0026quot;$http_user_agent\u0026quot; \u0026quot;$http_x_forwarded_for\u0026quot;'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; server_tokens off; keepalive_timeout 65; gzip on; include /etc/nginx/conf.d/*.conf; }  config for pass the PHP scripts to FastCGI server php-fpm using file socket in /etc/nginx/conf.d/default.conf server { listen 80; server_name localhost; #charset koi8-r; access_log /var/log/nginx/host.access.log main; root /usr/share/nginx/html; index index.html index.php index.htm; location / { } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } location ~ \\.php$ { fastcgi_pass unix:/var/run/php-fpm/php-fpm.sock; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } # session path # for mod_php, see /etc/httpd/conf.d/php.conf # for php-fpm, see /etc/php-fpm.d/www.conf  PHP config in /etc/php.ini cgi.fix_pathinfo=0  PHP-FPM config in /etc/php-fpm.d/www.conf listen = /var/run/php-fpm/php-fpm.sock listen.owner = nobody listen.group = nobody listen.mode = 0666 user = nginx group = nginx\nMariaDB config reference to https://twasa.github.io/post/my-db/\nVerify create /usr/share/nginx/html/info.php \u0026lt;?php phpinfo(); ?\u0026gt;  web permission for nginx chown -R nginx:nginx /usr/share/nginx/html/  selinux chcon --reference=/usr/share/nginx/html/index.html /usr/share/nginx/html/*  visit http://伺服器IP_or_FQDN/info.php to Verify "
},
{
	"uri": "https://blog.twasa.cf/",
	"title": "About",
	"tags": [],
	"description": "",
	"content": " Hugo is a static site engine written in Go.\nIt makes use of a variety of open source projects including:\n Cobra Viper J Walter Weatherman Cast  Learn more and contribute on GitHub.\nSetup Some fun facts about Hugo:\n Built in Go Loosely inspired by Jekyll Primarily developed by spf13 on the train while commuting to and from Manhattan. Coded in Vim using spf13-vim  Have questions or suggestions? Feel free to open an issue on GitHub or ask me on Twitter.\nThanks for reading!\n"
},
{
	"uri": "https://blog.twasa.cf/programing/python/python-virtualenv/",
	"title": "Python Virtualenv",
	"tags": ["Development", "Virtualenv"],
	"description": "",
	"content": " Virtualenv的好處  可以隔離函數庫需求不同的專案，讓它們不會互相影響。在建立並啟動虛擬環境後，透過 pip 安裝的套件會被放在虛擬環境中，專案就可以擁有一個獨立的環境。 在沒有權限的情況下安裝新套件 不同專案可以使用不同版本的相同套件 套件版本升級時不會影響其他專案  安裝 pip install virtualenv  建立專案資料夾 mkdir myproject cd myproject  初始化 virtualenv 虛擬環境名稱  activate the corresponding environment #linux . 虛擬環境名稱/bin/activate #windows 虛擬環境名稱\\scripts\\activate  這時候再裝要使用的Python Package pip install XXXX  回到實際環境 deactivate  若因為專案資料夾搬移或更名,請重新locate cd /path/to/your_project_new_dir virtualenv --relocatable your_virtualenv_name  "
},
{
	"uri": "https://blog.twasa.cf/programing/python/python-html-entity/",
	"title": "Python HTML Entity decode and encode",
	"tags": ["Development"],
	"description": "",
	"content": "–– coding: utf-8 – #decode import HTMLParser h = HTMLParser.HTMLParser() print h.unescape('\u0026amp;#35377;\u0026amp;#21151;\u0026amp;#33995;') 許功蓋 #encode a = u'許功蓋' a.encode('ascii', 'xmlcharrefreplace') '\u0026amp;#35377;\u0026amp;#21151;\u0026amp;#33995;'  "
},
{
	"uri": "https://blog.twasa.cf/post/flask-quick-start-guide/",
	"title": "Flask Quick Start Guide",
	"tags": ["Development"],
	"description": "",
	"content": " #Flask Quickstart\nRequirements  Python Pip virtualenv Flask : a web development framework Werkzeug：a toolkit for WSGI Jinja2：renders templates  virtualenv：multiple side-by-side installations of Python  安裝：pip install virtualenv 基本使用方式：  ### 先建立專案資料夾 mkdir myproject cd myproject ### 初始化 virtualenv env ### activate the corresponding environment . venv/bin/activate or venv\\scripts\\activate ### 這時候再裝Flask pip install Flask ### 回到實際環境 deactivate ### 若因為目錄搬移或更名 需重新locate cd /path/to/your_project_new_dir virtualenv --relocatable your_virtualenv_name  建立第一個app(記得啟動venv), save it as hello.py from flask import Flask app = Flask(__name__) @app.route('/') def hello_world(): return 'Hello, World!'  執行 # Unix Like: export FLASK_APP=hello.py python -m flask run * Running on http://127.0.0.1:5000/ # Windows: set FLASK_APP=hello.py python -m flask run --host=0.0.0.0 * Running on http://127.0.0.1:5000/ # Debug Mode: # export FLASK_DEBUG=1  Routing  beautiful URLs 程式範例：\n@app.route('/') def index(): return 'Index Page' @app.route('/hello') def hello(): return 'Hello, World'   Variable Rules  add variable parts to a URL ==string== accepts any text without a slash (the default) ==int== accepts integers ==float== like int but for floating point values ==path== like the default but also accepts slashes ==any== matches one of the items provided ==uuid== accepts UUID strings  @app.route('/user/\u0026lt;username\u0026gt;') def show_user_profile(username): # show the user profile for that user return 'User %s' % username @app.route('/post/\u0026lt;int:post_id\u0026gt;') def show_post(post_id): # show the post with the given id, the id is an integer return 'Post %d' % post_id  Unique URLs / Redirection Behavior 有斜線跟沒斜線的差異  helps search engines avoid indexing the same page twice 有斜線當網址輸入/projects or /projects/ 都會看的到相同結果(will cause Flask to redirect to the canonical URL with the trailing slash)  @app.route('/projects/') def projects(): return 'The project page' ## 沒斜線當輸入/about看的到結果，輸入/about/ 會顯示Not Found @app.route('/about') def about(): return 'The about page'  URL Building  build a URL to a specific function using url_for() function 好處 Reversing is often more descriptive than hard-coding the URLs. More importantly, it allows you to change URLs in one go, without having to remember to change URLs all over the place. URL building will handle escaping of special characters and Unicode data transparently for you, so you don’t have to deal with them. If your application is placed outside the URL root (say, in /myapplication instead of /), url_for() will handle that properly for you.\n examples:\n  \u0026gt;\u0026gt;\u0026gt; from flask import Flask, url_for \u0026gt;\u0026gt;\u0026gt; app = Flask(__name__) \u0026gt;\u0026gt;\u0026gt; @app.route('/') ... def index(): pass ... \u0026gt;\u0026gt;\u0026gt; @app.route('/login') ... def login(): pass ... \u0026gt;\u0026gt;\u0026gt; @app.route('/user/\u0026lt;username\u0026gt;') ... def profile(username): pass ... \u0026gt;\u0026gt;\u0026gt; with app.test_request_context(): ... print url_for('index') ... print url_for('login') ... print url_for('login', next='/') ... print url_for('profile', username='John Doe') ... / /login /login?next=/ /user/John%20Doe  HTTP Methods  By default, a route only answers to GET requests examples:  from flask import request @app.route('/login', methods=['GET', 'POST']) def login(): if request.method == 'POST': do_the_login() else: show_the_login_form()   GET: The browser tells the server to just get the information stored on that page and send it. This is probably the most common method. HEAD: The browser tells the server to get the information, but it is only interested in the headers, not the content of the page. An application is supposed to handle that as if a GET request was received but to not deliver the actual content. In Flask you don’t have to deal with that at all, the underlying Werkzeug library handles that for you. POST: The browser tells the server that it wants to post some new information to that URL and that the server must ensure the data is stored and only stored once. This is how HTML forms usually transmit data to the server. PUT: Similar to POST but the server might trigger the store procedure multiple times by overwriting the old values more than once. Now you might be asking why this is useful, but there are some good reasons to do it this way. Consider that the connection is lost during transmission: in this situation a system between the browser and the server might receive the request safely a second time without breaking things. With POST that would not be possible because it must only be triggered once. DELETE: Remove the information at the given location. OPTIONS: Provides a quick way for a client to figure out which methods are supported by this URL. Starting with Flask 0.6, this is implemented for you automatically.  Static Files To generate URLs for static files, use the special \u0026lsquo;static\u0026rsquo; endpoint name: url_for(\u0026lsquo;static\u0026rsquo;, filename=\u0026lsquo;style.css\u0026rsquo;) The file has to be stored on the filesystem as static/style.css.\nRendering Templates  用python產生HTML一點也不有趣而且是個累贅，所以Flask使用Jinja2 template engine來處理HTML 也可避免自己要做HTML escaping來維持App的安全 use the render_template method a simple example of how to render a template:  from flask import render_template @app.route('/hello/') @app.route('/hello/\u0026lt;name\u0026gt;') def hello(name=None): return render_template('hello.html', name=name)   Here is an example template:  \u0026lt;!doctype html\u0026gt; \u0026lt;title\u0026gt;Hello from Flask\u0026lt;/title\u0026gt; {% if name %} \u0026lt;h1\u0026gt;Hello {{ name }}!\u0026lt;/h1\u0026gt; {% else %} \u0026lt;h1\u0026gt;Hello, World!\u0026lt;/h1\u0026gt; {% endif %}  Flask will look for templates in the templates folder. So if your application is a module, this folder is next to that module, if it’s a package it’s actually inside your package: Case 1: a module: /application.py /templates /hello.html Case 2: a package: /application /__init__.py /templates /hello.html  The Request Object  request object most common operations  from flask import request @app.route('/login', methods=['POST', 'GET']) def login(): error = None if request.method == 'POST': if valid_login(request.form['username'], request.form['password']): return log_the_user_in(request.form['username']) else: error = 'Invalid username/password'  # the code below is executed if the request method was GET or the credentials were invalid return render_template('login.html', error=error) #To access parameters submitted in the URL (?key=value) you can use the args attribute: searchword = request.args.get('key', '')  File Uploads  set the enctype=\u0026ldquo;multipart/form-data\u0026rdquo; attribute on your HTML form 上傳的檔案可儲存在memory或filesystem it also has a save() method that allows you to store that file on the filesystem  from flask import request @app.route('/upload', methods=['GET', 'POST']) def upload_file(): if request.method == 'POST': f = request.files['the_file'] f.save('/var/www/uploads/uploaded_file.txt')   想取得檔案在client端為上傳之前的檔名可以存取 filename attribute，然而這是有可能被偽造的 如果你想使用client端為上傳之前的檔名，來存放到server上透過Werkzeug 提供的 secure_filename() function  from flask import request from werkzeug.utils import secure_filename @app.route('/upload', methods=['GET', 'POST']) def upload_file(): if request.method == 'POST': f = request.files['the_file'] f.save('/var/www/uploads/' + secure_filename(f.filename))  Cookies  Reading cookies:  from flask import request @app.route('/') def index(): username = request.cookies.get('username') # use cookies.get(key) instead of cookies[key] to not get a # KeyError if the cookie is missing.   Storing cookies:  from flask import make_response @app.route('/') def index(): resp = make_response(render_template(...)) resp.set_cookie('username', 'the username') return resp  Redirects and Errors  To redirect a user to another endpoint, use the redirect() function To abort a request early with an error code, use the abort() function  from flask import abort, redirect, url_for @app.route('/') def index(): return redirect(url_for('login')) @app.route('/login') def login(): abort(401) this_is_never_executed()  customize the error page, you can use the errorhandler() decorator from flask import render_template @app.errorhandler(404) def page_not_found(error): return render_template('page_not_found.html'), 404  About Responses  return value from a view function is automatically converted into a response object The logic that Flask applies to converting return values into response objects is as follows If a response object of the correct type is returned it’s directly returned from the view. If it’s a string, a response object is created with that data and the default parameters. If a tuple is returned the items in the tuple can provide extra information. Such tuples have to be in the form (response, status, headers) or (response, headers) where at least one item has to be in the tuple. The status value will override the status code and headers can be a list or dictionary of additional header values. If none of that works, Flask will assume the return value is a valid WSGI application and convert that into a response object.  ##get hold of the resulting response object inside the view use the make_response() function Imagine you have a view like this:\n@app.errorhandler(404) def not_found(error): return render_template('error.html'), 404  You just need to wrap the return expression with make_response() and get the response object to modify it, then return it:\n@app.errorhandler(404) def not_found(error): resp = make_response(render_template('error.html'), 404) resp.headers['X-Something'] = 'A value' return resp  Sessions  allows you to store information specific to a user from one request to the next implemented on top of cookies for you and signs the cookies cryptographically user could look at the contents of your cookie but not modify it examples:  from flask import Flask, session, redirect, url_for, escape, request app = Flask(__name__) @app.route('/') def index(): if 'username' in session: return 'Logged in as %s' % escape(session['username']) return 'You are not logged in' @app.route('/login', methods=['GET', 'POST']) def login(): if request.method == 'POST': session['username'] = request.form['username'] return redirect(url_for('index')) return ''' \u0026lt;form action=\u0026quot;\u0026quot; method=\u0026quot;post\u0026quot;\u0026gt; \u0026lt;p\u0026gt;\u0026lt;input type=text name=username\u0026gt; \u0026lt;p\u0026gt;\u0026lt;input type=submit value=Login\u0026gt; \u0026lt;/form\u0026gt; ''' @app.route('/logout') def logout(): # remove the username from the session if it's there session.pop('username', None) return redirect(url_for('index')) # set the secret key. keep this really secret: app.secret_key = 'A0Zr98j/3yX R~XHH!jmN]LWX/,?RT'  How to generate good secret keys in python import os os.urandom(24)  About cookie-base sessions size * A note on cookie-based sessions: Flask will take the values you put into the session object and serialize them into a cookie. If you are finding some values do not persist across requests, cookies are indeed enabled, and you are not getting a clear error message, check the size of the cookie in your page responses compared to the size supported by web browsers. *\nLogging * The attached logger is a standard logging Logger, so head over to the official logging documentation for more information. *\napp.logger.debug('A value for debugging') app.logger.warning('A warning occurred (%d apples)', 42) app.logger.error('An error occurred')  Hooking in WSGI Middlewares  If you want to add a WSGI middleware to your application you can wrap the internal WSGI application. For example if you want to use one of the middlewares from the Werkzeug package to work around bugs in lighttpd, you can do it like this: https://docs.python.org/library/logging.html  from werkzeug.contrib.fixers import LighttpdCGIRootFix app.wsgi_app = LighttpdCGIRootFix(app.wsgi_app)  Using Flask Extensions  Extensions are packages that help you accomplish common tasks. For example, Flask-SQLAlchemy provides SQLAlchemy support that makes it simple and easy to use with Flask. http://flask.pocoo.org/docs/0.11/extensions/#extensions  Deploying to a Web Server  http://flask.pocoo.org/docs/0.11/deploying/#deployment  "
},
{
	"uri": "https://blog.twasa.cf/post/hugo-how-to/",
	"title": "Hugo A Fast and Flexible Website Generator",
	"tags": [],
	"description": "",
	"content": " Quick Start Guide  Hugo download\n https://github.com/gohugoio/hugo/releases  Install Hugo\n https://gohugo.io/getting-started/installing/  Create a blog folder\n  hugo new site blog   go in to the blog folder and modify config.toml for your github url  baseURL = \u0026quot;https://\u0026lt;your-github-account\u0026gt;.github.io/\u0026quot;   create new page  hugo new about.md   create new post  hugo new post/first.md   install a theme  cd themes git clone https://github.com/xxx/xxx.git   localhost server preview  hugo server --buildDrafts --watch   references  https://gohugo.io/overview/introduction/  themes\n https://github.com/spf13/hugoThemes  deploy to github\n Create a blog git repository on GitHub Create a your-github-account.github.io repository on GitHub In your blog folder, run commands as below   git init git remote add origin git@github.com:\u0026lt;your-github-account\u0026gt;/blog.git git rm -r public git submodule add git@github.com:\u0026lt;your-github-account\u0026gt;/\u0026lt;your-github-account\u0026gt;.github.io.git public git add . git commit -m \u0026quot;Hugo content update\u0026quot; git push -u origin master hugo --buildDrafts cd public git add . git commit -m \u0026quot;Blog update\u0026quot; git push origin master   In your blog folder, create a batch file for one click deploy.  windows : save below content as .cmd   set workdir=%~dp0 set current=%date:~0,4%-%date:~5,2%-%date:~8,2% %time:~0,2%:%time:~3,2%:%time:~6,2% cd %workdir% git add -A git commit -m \u0026quot;Hugo content update %current%\u0026quot; git push -u origin master hugo --buildDrafts cd public git add -A git commit -m \u0026quot;Blog update %current%\u0026quot; git push origin master   linux : save below content as .sh  #!/bin/bash workdir=\u0026quot;$( cd \u0026quot;$( dirname \u0026quot;${BASH_SOURCE[0]}\u0026quot; )\u0026quot; \u0026amp;\u0026amp; pwd )\u0026quot; cd $workdir git add -A git commit -m \u0026quot;Hugo content update `date`\u0026quot; hugo --buildDrafts cd public git add -A git commit -m \u0026quot;Blog update `date`\u0026quot; git push origin master  "
},
{
	"uri": "https://blog.twasa.cf/post/mdadm-rescue/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "mdadm --assemble /dev/md6 /dev/sde3 mount iscsiadm history clear exit exit df df cat /proc/mdstat mount mdadm -Sf /dev/md6 mdadm -Af /dev/md6 /dev/sde3 cat /proc/mdstat synospace --map-file -d synocheckshare synocheckiscsitrg synocheckiscsitrg  "
},
{
	"uri": "https://blog.twasa.cf/tags/apache/",
	"title": "Apache",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/automation/",
	"title": "Automation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/aws/",
	"title": "Aws",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/cisco/",
	"title": "Cisco",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/cloud/",
	"title": "Cloud",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/command/",
	"title": "Command",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/commands/",
	"title": "Commands",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/container/",
	"title": "Container",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/db/",
	"title": "DBs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/database/",
	"title": "Database",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/devops/",
	"title": "DevOps",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/development/",
	"title": "Development",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/devops/",
	"title": "Devops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/devops/",
	"title": "Devops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/docker/",
	"title": "Docker",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/linux/",
	"title": "Linux",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/linux/",
	"title": "Linuxes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/logrotation/",
	"title": "Logrotation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/mariadb/",
	"title": "Mariadb",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/microsoft/",
	"title": "Microsoft",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/mongodb/",
	"title": "Mongodb",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/mysql/",
	"title": "Mysql",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/network/",
	"title": "Network",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/nginx/",
	"title": "Nginx",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/nosql/",
	"title": "Nosql",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/post/",
	"title": "Posts",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/powershell/",
	"title": "Powershell",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/programing/",
	"title": "Programings",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/python/",
	"title": "Python",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/regular-expression/",
	"title": "Regular Expression",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/selenium/",
	"title": "Selenium",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/shell/",
	"title": "Shell",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/tomcat/",
	"title": "Tomcat",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/tools/",
	"title": "Tools",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/version-control/",
	"title": "Version Control",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/tags/virtualenv/",
	"title": "Virtualenv",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/virtualization/",
	"title": "Virtualization",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.twasa.cf/categories/webserver/",
	"title": "Webserver",
	"tags": [],
	"description": "",
	"content": ""
}]